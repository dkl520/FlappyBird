import os
import numpy as np
import gymnasium as gym
import flappy_bird_gymnasium
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

# ================= ğŸš€ è¶…å‚æ•°é…ç½® =================
ENV_ID = "FlappyBird-v0"
LEARNING_RATE = 2.5e-4
GAMMA = 0.99  # æŠ˜æ‰£å› å­
LAMBDA = 0.95  # GAE å‚æ•°
EPS_CLIP = 0.2  # PPO æˆªæ–­èŒƒå›´
K_EPOCHS = 5  # æ¯æ¬¡æ›´æ–°å¾ªç¯æ¬¡æ•° (SB3ä¸­é€šå¸¸æ˜¯10ï¼Œè¿™é‡Œè®¾4-10å‡å¯)
BATCH_SIZE = 64  # å°æ‰¹é‡å¤§å°
UPDATE_TIMESTEP = 2048  # æ¯éš”å¤šå°‘æ­¥æ›´æ–°ä¸€æ¬¡ç½‘ç»œ (å¯¹åº” SB3 çš„ n_steps)
TOTAL_TIMESTEPS = 700_000
ENTROPY_COEF = 0.01  # ç†µç³»æ•°ï¼Œé¼“åŠ±æ¢ç´¢

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ“Œ ä½¿ç”¨è®¾å¤‡: {device}")


# ================= ğŸ§  1. å®šä¹‰ Actor-Critic ç½‘ç»œ =================
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(ActorCritic, self).__init__()

        # å…±äº«ç‰¹å¾æå–å±‚ (å¯é€‰ï¼Œä¹Ÿå¯ä»¥åˆ†å¼€)
        # æ¿€å…‰é›·è¾¾æ•°æ®æ˜¯ 1D å‘é‡ï¼Œç”¨ MLP å¤„ç†
        self.base_layer = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.Tanh(),
            nn.Linear(128, 128),
            nn.Tanh()
        )

        # Actor: è¾“å‡ºåŠ¨ä½œæ¦‚ç‡ (Logits)
        self.actor = nn.Sequential(
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )

        # Critic: è¾“å‡ºçŠ¶æ€ä»·å€¼ (Value)
        self.critic = nn.Linear(128, 1)

    def forward(self):
        raise NotImplementedError

    def act(self, state):
        """ç”¨äºåœ¨ç¯å¢ƒä¸­é‡‡æ ·åŠ¨ä½œ"""
        x = self.base_layer(state)
        action_probs = self.actor(x)
        dist = Categorical(action_probs)

        action = dist.sample()
        action_logprob = dist.log_prob(action)
        state_val = self.critic(x)

        return action.item(), action_logprob.item(), state_val.item()

    def evaluate(self, state, action):
        """ç”¨äºåœ¨æ›´æ–°æ—¶è¯„ä¼°æ—§åŠ¨ä½œçš„æ¦‚ç‡å’Œä»·å€¼"""
        x = self.base_layer(state)

        action_probs = self.actor(x)
        dist = Categorical(action_probs)

        action_logprobs = dist.log_prob(action)
        dist_entropy = dist.entropy()
        state_values = self.critic(x)

        return action_logprobs, state_values, dist_entropy


# ================= ğŸ› ï¸ 2. å®šä¹‰ PPO ç®—æ³•é€»è¾‘ =================
class PPO:
    def __init__(self, state_dim, action_dim):
        self.policy = ActorCritic(state_dim, action_dim).to(device)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=LEARNING_RATE)
        self.policy_old = ActorCritic(state_dim, action_dim).to(device)
        self.policy_old.load_state_dict(self.policy.state_dict())

        self.MseLoss = nn.MSELoss()

    def update(self, memory):
        # è½¬æ¢æ•°æ®ä¸º Tensor
        rewards = []
        discounted_reward = 0

        # --- è®¡ç®—è’™ç‰¹å¡æ´›å›æŠ¥ (Returns) æˆ– GAE ---
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„ Cost-to-Go (Return) è®¡ç®—ï¼Œç»“åˆ GAE æ•ˆæœæ›´å¥½ï¼Œ
        # ä¸ºäº†ä»£ç æ¸…æ™°ï¼Œè¿™é‡Œå…ˆè®¡ç®— Reward-to-Goç”¨äºè®¡ç®—ä¼˜åŠ¿
        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (GAMMA * discounted_reward)
            rewards.insert(0, discounted_reward)

        # å½’ä¸€åŒ–å›æŠ¥ (è¿™å¯¹æ”¶æ•›å¾ˆå…³é”®)
        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)

        # å°† list è½¬ä¸º tensor
        old_states = torch.squeeze(torch.stack(memory.states, dim=0)).detach().to(device)
        old_actions = torch.squeeze(torch.stack(memory.actions, dim=0)).detach().to(device)
        old_logprobs = torch.squeeze(torch.stack(memory.logprobs, dim=0)).detach().to(device)
        old_state_values = torch.squeeze(torch.stack(memory.state_values, dim=0)).detach().to(device)

        # è®¡ç®—ä¼˜åŠ¿å‡½æ•° (Advantage) = Return - Value
        # åœ¨æ ‡å‡†çš„ PPO ä¸­é€šå¸¸ä½¿ç”¨ GAEï¼Œè¿™é‡Œç®€åŒ–ä¸º returns - old_values
        advantages = rewards.detach() - old_state_values.detach()

        # --- PPO æ›´æ–°å¾ªç¯ (K epochs) ---
        for _ in range(K_EPOCHS):
            # è¯„ä¼°æ—§çŠ¶æ€å’ŒåŠ¨ä½œ
            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)
            state_values = torch.squeeze(state_values)

            # è®¡ç®—æ¯”ç‡ ratio (pi_theta / pi_theta_old)
            # exp(log_prob - old_log_prob) = prob / old_prob
            ratio = torch.exp(logprobs - old_logprobs)

            # --- æ ¸å¿ƒ Loss å…¬å¼ ---
            # 1. Surrogate Loss
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - EPS_CLIP, 1 + EPS_CLIP) * advantages
            loss_actor = -torch.min(surr1, surr2)

            # 2. Value Loss (MSE)
            loss_critic = self.MseLoss(state_values, rewards)

            # 3. Total Loss (åŠ ä¸Šç†µæ­£åˆ™é¡¹é¼“åŠ±æ¢ç´¢)
            loss = loss_actor + 0.5 * loss_critic - ENTROPY_COEF * dist_entropy

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()

        # æ›´æ–°æ—§ç­–ç•¥ç½‘ç»œ
        self.policy_old.load_state_dict(self.policy.state_dict())

    def save(self, checkpoint_path):
        torch.save(self.policy_old.state_dict(), checkpoint_path)

    def load(self, checkpoint_path):
        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=device))
        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=device))


# ================= ğŸ“¦ 3. ç®€å•çš„ç»éªŒå›æ”¾ç¼“å†²åŒº =================
class Memory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []
        self.state_values = []

    def clear(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]
        del self.state_values[:]


# ================= ğŸƒ è®­ç»ƒæµç¨‹ =================
def train():
    print("ğŸš€ å¼€å§‹æ‰‹å†™ PPO è®­ç»ƒ...")

    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(ENV_ID, use_lidar=True, background=None)

    # è·å–ç»´åº¦
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    memory = Memory()
    ppo_agent = PPO(state_dim, action_dim)

    time_step = 0
    i_episode = 0

    while time_step < TOTAL_TIMESTEPS:
        state, _ = env.reset()
        current_ep_reward = 0
        done = False

        while not done:
            # 1. é€‰æ‹©åŠ¨ä½œ
            # æ³¨æ„ï¼šstate éœ€è¦è½¬ä¸º tensor ä¸”å¢åŠ  batch ç»´åº¦
            state_tensor = torch.FloatTensor(state).to(device)
            action, logprob, val = ppo_agent.policy_old.act(state_tensor)

            # 2. æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

            # 3. å­˜å‚¨æ•°æ®åˆ° Buffer
            memory.states.append(state_tensor)
            memory.actions.append(torch.tensor(action).to(device))
            memory.logprobs.append(torch.tensor(logprob).to(device))
            memory.state_values.append(torch.tensor(val).to(device))
            memory.rewards.append(reward)
            memory.is_terminals.append(terminated)  # æ³¨æ„è¿™é‡Œç”¨ terminated æ¯”è¾ƒå¥½

            state = next_state
            current_ep_reward += reward
            time_step += 1

            # 4. å¦‚æœè¾¾åˆ°äº†æ›´æ–°æ­¥æ•°ï¼Œè¿›è¡Œ PPO æ›´æ–°
            if time_step % UPDATE_TIMESTEP == 0:
                print(f"ğŸ”„ Step {time_step}: æ›´æ–°ç­–ç•¥ç½‘ç»œ...")
                ppo_agent.update(memory)
                memory.clear()

            if done:
                break

        i_episode += 1

        # ç®€å•æ‰“å°æ—¥å¿—
        if i_episode % 20 == 0:
            print(
                f"Episode: {i_episode} \t Timestep: {time_step} \t Reward: {current_ep_reward:.2f} \t Score: {info.get('score', 0)}")

        # å®šæœŸä¿å­˜
        if time_step % 50000 == 0:
            os.makedirs("manual_models", exist_ok=True)
            ppo_agent.save(f"manual_models/ppo_flappy_{time_step}.pth")

    print("âœ… è®­ç»ƒç»“æŸ")
    ppo_agent.save("manual_models/ppo_flappy_final.pth")
    env.close()


# ================= ğŸ® æµ‹è¯•æµç¨‹ =================
def test():
    print("ğŸ‘€ åŠ è½½æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
    env = gym.make(ENV_ID, render_mode="human", use_lidar=True)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    ppo_agent = PPO(state_dim, action_dim)
    model_path = "manual_models/ppo_flappy_final.pth"

    if not os.path.exists(model_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹: {model_path}ï¼Œè¯·å…ˆè®­ç»ƒã€‚")
        return

    ppo_agent.load(model_path)

    for ep in range(5):
        state, _ = env.reset()
        done = False
        score = 0
        while not done:
            state_tensor = torch.FloatTensor(state).to(device)
            # æµ‹è¯•æ—¶å–ç¡®å®šæ€§åŠ¨ä½œï¼šé€‰æ¦‚ç‡æœ€å¤§çš„
            # ä½†æ‰‹å†™actå‡½æ•°é€šå¸¸æ˜¯é‡‡æ ·çš„ã€‚ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œè¿™é‡Œæˆ‘ä»¬è¿˜æ˜¯ç”¨acté‡‡æ ·ï¼Œ
            # çœŸæ­£ä¸¥è°¨çš„æµ‹è¯•åº”è¯¥å– actor è¾“å‡º logits æœ€å¤§çš„é‚£ä¸ª indexã€‚
            with torch.no_grad():
                action_probs = ppo_agent.policy.actor(state_tensor)
                action = torch.argmax(action_probs).item()  # è´ªå©ªç­–ç•¥

            state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            score = info.get('score', 0)

        print(f"Episode {ep + 1} Score: {score}")

    env.close()


if __name__ == "__main__":
    # åˆ‡æ¢è¿™é‡Œæ¥è®­ç»ƒæˆ–æµ‹è¯•
    train()
    # test()