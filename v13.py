import os
import time
import numpy as np
import gymnasium as gym
import flappy_bird_gymnasium
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.monitor import Monitor

# ================= ğŸš€ è¶…å‚æ•°é…ç½® =================
ENV_ID = "FlappyBird-v0"
MODEL_DIR = "manual_models"
MODEL_NAME = "ppo_flappy_final13"  # ä¸å¸¦åç¼€
TOTAL_TIMESTEPS = 1_000_000
DEVICE = "cpu"  # ä¿æŒä½ åŸæ¥çš„è®¾ç½®


# ================= ğŸ›¡ï¸ å®‰å…¨å¥–åŠ±åŒ…è£…å™¨ (ä¿ç•™åŸé€»è¾‘) =================
class StrictSafetyWrapper(gym.Wrapper):
    def __init__(self, env, safe_dist=0.20):
        super().__init__(env)
        self.safe_dist = safe_dist

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)

        # SB3 çš„ç¯å¢ƒé€šå¸¸ä¼šè‡ªåŠ¨å¤„ç† obsï¼Œä½†åœ¨å•ç¯å¢ƒ Wrapper ä¸­ obs è¿˜æ˜¯ numpy æ•°ç»„
        # ä½ çš„é€»è¾‘ï¼šæƒ©ç½šè´´ç®¡é£è¡Œ
        if np.min(obs) < self.safe_dist:
            reward += 0.05

        return obs, reward, terminated, truncated, info


# ================= ğŸƒ è®­ç»ƒæµç¨‹ =================
def train():
    print("ğŸš€ å‡†å¤‡å¼€å§‹è®­ç»ƒ (Stable-Baselines3 ç‰ˆ)...")

    # 1. åˆ›å»ºç¯å¢ƒ (Monitor ç”¨äºè®°å½•æ•°æ®ç»™ SB3)
    env = gym.make(ENV_ID, use_lidar=True, background=None)
    env = StrictSafetyWrapper(env, safe_dist=0.20)
    env = Monitor(env)

    # 2. è·¯å¾„å¤„ç†
    os.makedirs(MODEL_DIR, exist_ok=True)
    model_path = os.path.join(MODEL_DIR, MODEL_NAME)
    checkpoint_path = os.path.join(MODEL_DIR, "checkpoints")

    # 3. æ–­ç‚¹ç»­è®­é€»è¾‘
    # SB3 ä¿å­˜çš„æ¨¡å‹åç¼€æ˜¯ .zipï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æ£€æŸ¥ .zip æ–‡ä»¶
    if os.path.exists(f"{model_path}.zip"):
        print(f"ğŸ”„ å‘ç°ä¸Šæ¬¡è®­ç»ƒæ¨¡å‹: {model_path}.zip")
        # åŠ è½½æ—§æ¨¡å‹ï¼Œå¹¶ç»‘å®šå½“å‰ç¯å¢ƒ
        model = PPO.load(model_path, env=env, device=DEVICE)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å°†åœ¨è¯¥æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒ (Resume Training)")
    else:
        print("ğŸ†• æœªæ‰¾åˆ°å·²æœ‰æ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ (Start From Scratch)")
        # åˆå§‹åŒ–æ–°æ¨¡å‹ (å‚æ•°æ˜ å°„ä½ çš„åŸé…ç½®)
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=2.5e-4,
            n_steps=2048,  # UPDATE_TIMESTEP
            batch_size=64,
            n_epochs=10,  # K_EPOCHS
            gamma=0.99,
            gae_lambda=0.95,  # LAMBDA
            clip_range=0.2,  # EPS_CLIP
            ent_coef=0.01,
            policy_kwargs=dict(net_arch=[128, 128]),  # å¯¹åº”ä½ çš„ä¸¤ä¸ª 128 å±‚
            verbose=1,
            device=DEVICE,
            tensorboard_log="./ppo_flappy_tensorboard/"
        )

    # 4. å›è°ƒå‡½æ•°ï¼šå®šæœŸä¿å­˜ (æ›¿ä»£ä½ åŸæ¥çš„ if time_step % 50000 == 0)
    checkpoint_callback = CheckpointCallback(
        save_freq=50000,
        save_path=MODEL_DIR,
        name_prefix="ppo_flappy"
    )

    # 5. å¼€å§‹è®­ç»ƒ (progress_bar=True è‡ªå¸¦ä½ çš„è¿›åº¦æ¡éœ€æ±‚)
    try:
        model.learn(
            total_timesteps=TOTAL_TIMESTEPS,
            callback=checkpoint_callback,
            progress_bar=True,
            reset_num_timesteps=False  # ç»­è®­æ—¶ä¸é‡ç½®æ­¥æ•°è®¡æ•°å™¨
        )
        # æœ€ç»ˆä¿å­˜
        model.save(model_path)
        print("âœ… è®­ç»ƒç»“æŸ")
        print(f"ğŸ“ æœ€ç»ˆæ¨¡å‹ä½ç½®: {os.path.abspath(model_path)}.zip")

    except KeyboardInterrupt:
        print("\nğŸ›‘ æ•è·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜æ¨¡å‹...")
        model.save(model_path)
        print("âœ… å·²ä¿å­˜å½“å‰è¿›åº¦")


# ================= ğŸ‘€ æµ‹è¯•æµç¨‹ =================
def test():
    print("ğŸ‘€ åŠ è½½æ¨¡å‹è¿›è¡Œæµ‹è¯• (æ— å°½æ¨¡å¼)...")

    # è·¯å¾„
    model_path = os.path.join(MODEL_DIR, MODEL_NAME)
    if not os.path.exists(f"{model_path}.zip"):
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹: {model_path}.zipï¼Œè¯·å…ˆè¿è¡Œ train() è¿›è¡Œè®­ç»ƒã€‚")
        return

    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ (Render Mode)
    env = gym.make(ENV_ID, render_mode="human", use_lidar=True, background="night")

    # åŠ è½½æ¨¡å‹
    model = PPO.load(model_path, device=DEVICE)

    episode_cnt = 0

    while True:
        episode_cnt += 1
        obs, _ = env.reset()
        done = False
        score = 0
        step_cnt = 0

        while not done:
            # predict è¿”å› (action, state)ï¼Œè¿™é‡Œåªéœ€è¦ action
            # deterministic=True è®©æµ‹è¯•è¡¨ç°æ›´ç¨³å®š
            action, _ = model.predict(obs, deterministic=True)

            obs, reward, terminated, truncated, info = env.step(action)
            score = info.get('score', 0)
            step_cnt += 1

            done = terminated or truncated

        print(f"Episode {episode_cnt} | Score: {score} | Steps: {step_cnt}")

        # æš‚åœ 1 ç§’è®©ä½ çœ‹æ¸… (ä¿ç•™ä½ çš„ä¹ æƒ¯)
        time.sleep(1)

    env.close()


if __name__ == "__main__":
    # train() # è®­ç»ƒæ¨¡å¼
    test()  # æµ‹è¯•æ¨¡å¼