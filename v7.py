import gymnasium as gym
import flappy_bird_gymnasium
import torch
import os
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.evaluation import evaluate_policy

# ==========================================
# âš™ï¸ å…¨å±€é…ç½®
# ==========================================
MODELS_DIR = "models/flappy_ppo_hard"  # æ”¹ä¸ªåå­—ï¼ŒåŒºåˆ†ä¹‹å‰çš„ç‰ˆæœ¬
LOG_DIR = "logs/flappy_ppo_hard"
BEST_MODEL_NAME = "best_model"
FINAL_MODEL_NAME = "last_run_model"

N_ENVS = 4
TOTAL_TIMESTEPS = 1_000_000

os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


# ==========================================
# ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šè‡ªå®šä¹‰â€œä¸¥æ ¼æ¨¡å¼â€åŒ…è£…å™¨
# ==========================================
class StrictSafetyWrapper(gym.Wrapper):
    """
    è¿™æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æ•™ç»ƒï¼š
    1. å¦‚æœç”±äºç¦»ç®¡å­å¤ªè¿‘è€Œè¿‡å…³ï¼Œæ²¡æ”¶å¤§éƒ¨åˆ†å¥–åŠ±ã€‚
    2. æ¯ä¸€å¸§å¦‚æœç¦»éšœç¢ç‰©å¤ªè¿‘ï¼Œç»™äºˆå¾®å°æƒ©ç½šã€‚
    """

    def __init__(self, env, safe_dist=0.14):
        super().__init__(env)
        self.safe_dist = safe_dist  # å®‰å…¨è·ç¦»é˜ˆå€¼ (0.0 - 1.0)

    def step(self, action):
        # è·å–åŸå§‹ç¯å¢ƒçš„åé¦ˆ
        # obs åœ¨ use_lidar=True æ—¶ï¼Œæ˜¯ä¸€ä¸ªåŒ…å« 180 ä¸ªé›·è¾¾æ•°æ®çš„æ•°ç»„
        # æ•°å€¼è¶Šå°ï¼Œä»£è¡¨ç¦»éšœç¢ç‰©è¶Šè¿‘
        obs, reward, terminated, truncated, info = self.env.step(action)

        # === ğŸ˜ˆ é­”æ”¹å¥–åŠ±é€»è¾‘ ===

        # 1. è·å–å½“å‰æœ€è¿‘éšœç¢ç‰©çš„è·ç¦»
        # obs é‡Œçš„æ•°æ®é€šå¸¸æ˜¯å½’ä¸€åŒ–çš„è·ç¦»
        min_distance = np.min(obs)

        # 2. æƒ©ç½šâ€œè´´è„¸é£è¡Œâ€ (Proximity Penalty)
        # å¦‚æœç¦»ä»»ä½•ä¸œè¥¿å¤ªè¿‘ï¼ˆå°äºé˜ˆå€¼ï¼‰ï¼Œæ¯ä¸€å¸§éƒ½æ‰£ä¸€ç‚¹ç‚¹åˆ†
        # è¿™ä¼šé€¼è¿«é¸Ÿæ—¶åˆ»ä¿æŒåœ¨ç©ºæ—·åœ°å¸¦
        if min_distance < self.safe_dist:
            reward -= 0.05  # å¾®å°æƒ©ç½šï¼Œä¸è¦æ‰£å¤ªå¤šï¼Œå¦åˆ™å®ƒä¼šé€‰æ‹©è‡ªæ€

        # 3. æƒ©ç½šâ€œæƒŠé™©è¿‡å…³â€
        # é»˜è®¤è¿‡ç®¡å¥–åŠ±é€šå¸¸æ˜¯ 1.0 (å…·ä½“çœ‹åº“ç‰ˆæœ¬ï¼Œé€šå¸¸é€šè¿‡ info['score'] å˜åŒ–åˆ¤æ–­ä¹Ÿå¯ä»¥)
        # è¿™é‡Œæˆ‘ä»¬ç®€å•å‡è®¾ reward > 0.5 å°±æ˜¯è¿‡ç®¡äº†
        if reward >= 1.0:
            if min_distance < self.safe_dist:
                # åˆšæ‰è™½ç„¶è¿‡ç®¡äº†ï¼Œä½†æ˜¯ç¦»ç®¡å­å¤ªè¿‘äº†ï¼
                # æŠŠå¥–åŠ±æ‰“æŠ˜ï¼Œåªç»™ 0.2 åˆ†
                reward = 0.2
                # æˆ–è€…ï¼šreward -= 0.8

        return obs, reward, terminated, truncated, info


# ==========================================
# ğŸ› ï¸ ç¯å¢ƒæ„å»ºå‡½æ•°
# ==========================================
def make_env():
    # 1. åŸºç¡€ç¯å¢ƒ
    env = gym.make("FlappyBird-v0", render_mode=None, use_lidar=True)

    # 2. ğŸ”¥ å¥—ä¸Šæˆ‘ä»¬çš„ä¸¥æ ¼æ•™ç»ƒåŒ…è£…å™¨
    env = StrictSafetyWrapper(env, safe_dist=0.15)  # 0.2 è¡¨ç¤ºé›·è¾¾æ¢æµ‹è·ç¦»çš„ 20%

    return env


# ==========================================
# ğŸ‹ï¸â€â™‚ï¸ è®­ç»ƒä¸»å‡½æ•° (ä¿æŒä¸å˜ï¼Œé€»è¾‘é€šç”¨)
# ==========================================
def train():
    print(f"\n>>> [ä¸¥æ ¼æ¨¡å¼] å¯åŠ¨è®­ç»ƒï¼Œå¦‚æœé£å¾—å¤ªè´´è¿‘ç®¡å­ä¼šè¢«æ‰£åˆ†ï¼")
    print(f">>> ç›®æ ‡æ­¥æ•°: {TOTAL_TIMESTEPS}")

    env = make_vec_env(make_env, n_envs=N_ENVS, monitor_dir=LOG_DIR)
    eval_env = make_vec_env(make_env, n_envs=1)

    # è½½å…¥å†å²æœ€ä½³åˆ†æ•°çš„é€»è¾‘
    best_model_path = os.path.join(MODELS_DIR, f"{BEST_MODEL_NAME}.zip")
    historical_best_score = -np.inf

    if os.path.exists(best_model_path):
        print(f">>> ğŸ† å‘ç°å†å²æ¨¡å‹ï¼Œæ­£åœ¨è¯„ä¼°...")
        try:
            temp_model = PPO.load(best_model_path)
            mean_reward, _ = evaluate_policy(temp_model, eval_env, n_eval_episodes=5)
            historical_best_score = mean_reward
            print(f">>> ğŸ“Š å†å²æœ€é«˜åˆ†: {historical_best_score:.2f}")
            del temp_model
        except:
            pass

    # æ¨¡å‹å®šä¹‰
    final_model_path = os.path.join(MODELS_DIR, f"{FINAL_MODEL_NAME}.zip")
    if os.path.exists(final_model_path):
        print(">>> â™»ï¸ ç»§ç»­è®­ç»ƒ...")
        model = PPO.load(final_model_path, env=env)
    else:
        print(">>> âœ¨ æ–°å»ºæ¨¡å‹...")
        model = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            tensorboard_log=LOG_DIR,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            policy_kwargs=dict(net_arch=dict(pi=[128, 128], vf=[128, 128]), activation_fn=torch.nn.Tanh),
        )

    # å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=MODELS_DIR,
        log_path=LOG_DIR,
        eval_freq=10000,
        n_eval_episodes=5,
        deterministic=True,
        render=False
    )
    eval_callback.best_mean_reward = historical_best_score

    # å¼€å§‹è®­ç»ƒ
    try:
        model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=eval_callback, progress_bar=True,
                    reset_num_timesteps=False)
    except KeyboardInterrupt:
        print(">>> ä¸­æ–­ä¿å­˜...")

    model.save(final_model_path)
    env.close()
    eval_env.close()


# ==========================================
# ğŸ® æµ‹è¯•å‡½æ•°
# ==========================================
import  time
def test():
    # ... (åŠ è½½æ¨¡å‹è·¯å¾„éƒ¨åˆ†ä¿æŒä¸å˜) ...
    load_path = os.path.join(MODELS_DIR, f"{BEST_MODEL_NAME}.zip")
    if not os.path.exists(load_path):
        load_path = os.path.join(MODELS_DIR, f"{FINAL_MODEL_NAME}.zip")

    if not os.path.exists(load_path):
        print("âŒ æ— æ¨¡å‹")
        return

    print(f">>> ğŸ® æ­£åœ¨åŠ è½½æ¨¡å‹: {load_path}")
    print(f">>> ğŸ•µï¸ æ­»äº¡æš‚åœæ¨¡å¼å·²å¼€å¯ï¼šæ’æ­»åè¯·çœ‹ç”»é¢ï¼ŒæŒ‰å›è½¦ç»§ç»­ï¼")

    # 1. åˆ›å»ºç¯å¢ƒ
    env = gym.make("FlappyBird-v0", render_mode="human", use_lidar=True)
    model = PPO.load(load_path)

    for ep in range(10):  # æµ‹è¯• 10 å±€
        obs, _ = env.reset()
        done = False
        start_time = time.time()

        print(f"\nğŸ¬ ç¬¬ {ep + 1} å±€å¼€å§‹...")

        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)

            # æ— é™æ¨¡å¼é€»è¾‘
            done = terminated

            # å¦‚æœè§¦å‘ Gym çš„æ—¶é—´é™åˆ¶ï¼Œè¿™é‡Œå¿½ç•¥å®ƒï¼Œåªçœ‹æ’æ­»
            if truncated:
                pass

            # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šæ­»äº¡æš‚åœ ğŸ”¥ğŸ”¥ğŸ”¥
            if terminated:
                # è®¡ç®—è¿™å±€é£äº†å¤šä¹…
                duration = time.time() - start_time
                final_score = info.get('score', 0)

                print(f"ğŸ›‘ [æ’è½¦ç¬é—´]ï¼")
                print(f"   åˆ†æ•°: {final_score} | å­˜æ´»: {duration:.2f}ç§’")
                print(f"   ğŸ‘€ è¯·æ£€æŸ¥æ¸¸æˆçª—å£ï¼Œçœ‹çœ‹åˆ°åº•æ’åˆ°äº†å“ªé‡Œï¼ˆé¡¶éƒ¨ï¼Ÿåº•éƒ¨ï¼Ÿç®¡å­è¾¹ç¼˜ï¼Ÿï¼‰")

                # â¸ï¸ è¿™é‡Œä¼šè®©ç¨‹åºå¡ä½ï¼Œç›´åˆ°ä½ æŒ‰ä¸‹å›è½¦
                input("ğŸ‘‰ æŒ‰ [å›è½¦é”® Enter] å¼€å§‹ä¸‹ä¸€å±€...")

    env.close()


if __name__ == "__main__":
    # train()
    test()