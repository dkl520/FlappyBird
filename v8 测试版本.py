# å¯¼å…¥å¿…è¦çš„åº“
import gymnasium as gym
import flappy_bird_gymnasium
import torch
import os
import numpy as np
import time
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.evaluation import evaluate_policy

# ==========================================
# âš™ï¸ å…¨å±€é…ç½®
# ==========================================
MODELS_DIR = "models/flappy_ppo_finalv8"
LOG_DIR = "logs/flappy_ppo_finalv8"
BEST_MODEL_NAME = "best_modelv8"
FINAL_MODEL_NAME = "last_run_modelv8"

N_ENVS = 8  # CPU æ ¸å¿ƒæ•°å…è®¸çš„è¯å»ºè®®ç”¨ 8ï¼Œå¦åˆ™ 4
TOTAL_TIMESTEPS = 1_500_000

os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


# ==========================================
# ğŸ§  æ ¸å¿ƒæ”¹è¿›ï¼šåŸºäºåŸå§‹å‘é‡çš„è§‚æµ‹åŒ…è£…å™¨ (å·²ä¿®æ­£ç´¢å¼•)
# ==========================================
class SmartObsWrapper(gym.Wrapper):
    """
    ä¿®å¤ç‰ˆï¼šæ­£ç¡®è§£æ FlappyBird-v0 çš„ 12 ç»´è§‚æµ‹æ•°ç»„
    ç´¢å¼•è¯´æ˜:
    0: æœ€åä¸€ä¸ªç®¡é“çš„æ°´å¹³ä½ç½®
    1: ç©å®¶çš„å‚ç›´ä½ç½®
    2: ç©å®¶çš„å‚ç›´é€Ÿåº¦
    3: ä¸‹ä¸€ä¸ªç®¡é“çš„æ°´å¹³ä½ç½®
    4: ä¸‹ä¸€ä¸ªä¸Šæ–¹ç®¡é“çš„å‚ç›´ä½ç½®
    5: ä¸‹ä¸€ä¸ªä¸‹æ–¹ç®¡é“çš„å‚ç›´ä½ç½®
    6: ä¸‹ä¸‹ä¸ªç®¡é“çš„æ°´å¹³ä½ç½®
    7: ä¸‹ä¸‹ä¸ªä¸Šæ–¹ç®¡é“çš„å‚ç›´ä½ç½®
    8: ä¸‹ä¸‹ä¸ªä¸‹æ–¹ç®¡é“çš„å‚ç›´ä½ç½®
    9: ç©å®¶çš„æ—‹è½¬è§’åº¦
    10: ? (å…¶ä»–ä¿¡æ¯)
    11: ? (å…¶ä»–ä¿¡æ¯)
    """

    def __init__(self, env):
        super().__init__(env)
        # è¾“å‡ºï¼š3ç»´å‘é‡ [dy, vel, dx]
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(3,), dtype=np.float32
        )

    def reset(self, **kwargs):
        # åŸå§‹ç¯å¢ƒè¿”å›çš„ raw_obs æ˜¯ä¸€ä¸ª 12 ç»´å‘é‡
        raw_obs, info = self.env.reset(**kwargs)
        return self._process_obs(raw_obs), info

    def step(self, action):
        raw_obs, reward, terminated, truncated, info = self.env.step(action)
        new_obs = self._process_obs(raw_obs)
        return new_obs, reward, terminated, truncated, info

    def _process_obs(self, raw_obs):
        """
        ä» FlappyBird-v0 çš„ 12 ç»´è§‚æµ‹ä¸­æå–å…³é”®ä¿¡æ¯
        """
        # 1. æå–åŸå§‹æ•°æ® (å·²ä¿®æ­£ç´¢å¼•)
        pipe_dist = raw_obs[3]  # ä¸‹ä¸€ä¸ªç®¡é“çš„æ°´å¹³ä½ç½®
        pipe_top = raw_obs[4]   # ä¸‹ä¸€ä¸ªä¸Šæ–¹ç®¡é“çš„å‚ç›´ä½ç½®
        pipe_bottom = raw_obs[5] # ä¸‹ä¸€ä¸ªä¸‹æ–¹ç®¡é“çš„å‚ç›´ä½ç½®
        bird_y = raw_obs[1]     # ç©å®¶çš„å‚ç›´ä½ç½® (ä¿®æ­£: ç´¢å¼•1)
        bird_vel = raw_obs[2]   # ç©å®¶çš„å‚ç›´é€Ÿåº¦ (ä¿®æ­£: ç´¢å¼•2)

        # 2. è®¡ç®—ç‰¹å¾
        pipe_center_y = (pipe_top + pipe_bottom) / 2.0

        # A. å‚ç›´åå·® (dy)
        dy = (bird_y - pipe_center_y) / 512.0  # å±å¹•é«˜åº¦512

        # B. é€Ÿåº¦ (vel)
        vel = bird_vel / 10.0  # å¤§è‡´æœ€å¤§é€Ÿåº¦10

        # C. æ°´å¹³è·ç¦» (dx)
        dx = pipe_dist / 288.0  # å±å¹•å®½åº¦288

        return np.array([dy, vel, dx], dtype=np.float32)


# ==========================================
# ğŸ›¡ï¸ å¥–åŠ±å‡½æ•°ï¼šSmartRewardWrapper (é€»è¾‘ä¿®æ­£)
# ==========================================
class SmartRewardWrapper(gym.Wrapper):
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)

        # obs å·²ç»æ˜¯ [dy, vel, dx]
        dy = obs[0]
        dx = obs[2]

        # 1. åŸºç¡€å­˜æ´»å¥–åŠ±
        reward += 0.01

        # 2. æ™ºèƒ½å±…ä¸­å¥–åŠ± (åªåœ¨ç®¡å­åœ¨å°é¸Ÿå‰é¢æ—¶åº”ç”¨)
        if dx > 0:  # ä»…å½“ç®¡å­åœ¨å°é¸Ÿå‰é¢
            center_bonus = 0.2 * (1.0 - abs(dy) * 4)
            reward += max(0, center_bonus)

        # 3. æ’å‡»æƒ©ç½š
        if terminated:
            reward -= 1.0

        return obs, reward, terminated, truncated, info


# ==========================================
# ğŸ› ï¸ ç¯å¢ƒç»„è£…
# ==========================================
def make_env():
    # åŸºç¡€ç¯å¢ƒ (å·²ç§»é™¤æ— æ•ˆçš„ use_lidar å‚æ•°)
    env = gym.make("FlappyBird-v0", render_mode=None)
    # 1. åŒ…è£…è§‚æµ‹
    env = SmartObsWrapper(env)
    # 2. åŒ…è£…å¥–åŠ±
    env = SmartRewardWrapper(env)
    return env


# ==========================================
# ğŸ‹ï¸â€â™‚ï¸ è®­ç»ƒæµç¨‹
# ==========================================
def train():
    print(f"\n>>> [æœ€ç»ˆä¿®å¤ç‰ˆ] å¯åŠ¨è®­ç»ƒ...")
    print(f">>> æ¨¡å¼: CPU (åŠ é€Ÿ) | è§‚æµ‹: [dy, vel, dx]")

    # é‡æ–°åˆ›å»ºç¯å¢ƒï¼Œé¿å…æ—§ç¼“å­˜é—®é¢˜
    env = make_vec_env(make_env, n_envs=N_ENVS, monitor_dir=LOG_DIR)
    eval_env = make_vec_env(make_env, n_envs=1)

    model = PPO(
        "MlpPolicy",
        env,
        device="cpu",  # å¼ºåˆ¶ CPU
        verbose=1,
        tensorboard_log=LOG_DIR,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        policy_kwargs=dict(
            net_arch=dict(pi=[64, 64], vf=[64, 64]),
            activation_fn=torch.nn.Tanh,
        ),
    )

    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=MODELS_DIR,
        log_path=LOG_DIR,
        eval_freq=20000 // N_ENVS,
        n_eval_episodes=5,
        deterministic=True,
        render=False,
    )

    try:
        model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=eval_callback, progress_bar=True)
    except KeyboardInterrupt:
        print("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜...")

    model.save(os.path.join(MODELS_DIR, FINAL_MODEL_NAME))
    env.close()
    eval_env.close()
    print("è®­ç»ƒç»“æŸï¼")


# ==========================================
# ğŸ® æµ‹è¯•æµç¨‹
# ==========================================
def test():
    model_path = os.path.join(MODELS_DIR, f"{BEST_MODEL_NAME}.zip")
    if not os.path.exists(model_path):
        model_path = os.path.join(MODELS_DIR, f"{FINAL_MODEL_NAME}.zip")

    if not os.path.exists(model_path):
        print("âŒ æ²¡æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ train()")
        return

    print(f">>> ğŸ® åŠ è½½æ¨¡å‹: {model_path}")

    env = gym.make("FlappyBird-v0", render_mode="human")
    env = SmartObsWrapper(env)

    model = PPO.load(model_path, device="cpu")

    for ep in range(10):
        obs, _ = env.reset()
        done = False
        print(f"--- ç¬¬ {ep + 1} å±€ ---")

        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

            if terminated:
                print(f"ğŸ’€ æ­»äº¡! åˆ†æ•°: {info.get('score', 0)}")
                time.sleep(1.0)

    env.close()


if __name__ == "__main__":
    train()
    # test()  # æµ‹è¯•æ—¶å–æ¶ˆæ³¨é‡Š