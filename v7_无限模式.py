# å¯¼å…¥å¿…è¦çš„åº“
import gymnasium as gym
import flappy_bird_gymnasium  # Flappy Bird çš„ Gymnasium ç¯å¢ƒ
import torch
import os
import numpy as np
from stable_baselines3 import PPO  # ä½¿ç”¨ PPO ç®—æ³•
from stable_baselines3.common.env_util import make_vec_env  # ç”¨äºå¹¶è¡Œç¯å¢ƒ
from stable_baselines3.common.callbacks import EvalCallback  # è¯„ä¼°å›è°ƒ
from stable_baselines3.common.evaluation import evaluate_policy  # ç”¨äºè¯„ä¼°ç­–ç•¥

# ==========================================
# âš™ï¸ å…¨å±€é…ç½®ï¼šå®šä¹‰è®­ç»ƒå’Œæ—¥å¿—è·¯å¾„ã€è¶…å‚æ•°ç­‰
# ==========================================
MODELS_DIR = "models/flappy_ppo_hard"  # æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆåŒºåˆ«äºæ™®é€šç‰ˆæœ¬ï¼‰
LOG_DIR = "logs/flappy_ppo_hard"       # TensorBoard æ—¥å¿—ç›®å½•
BEST_MODEL_NAME = "best_model"         # æœ€ä½³æ¨¡å‹æ–‡ä»¶åï¼ˆè‡ªåŠ¨ä¿å­˜ï¼‰
FINAL_MODEL_NAME = "last_run_model"    # æœ€ç»ˆæ¨¡å‹æ–‡ä»¶åï¼ˆæ¯æ¬¡è®­ç»ƒç»“æŸä¿å­˜ï¼‰

N_ENVS = 4                             # å¹¶è¡Œç¯å¢ƒæ•°é‡ï¼ˆæå‡é‡‡æ ·æ•ˆç‡ï¼‰
TOTAL_TIMESTEPS = 1_000_000            # æ€»è®­ç»ƒæ­¥æ•°

# åˆ›å»ºæ¨¡å‹å’Œæ—¥å¿—ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


# ==========================================
# ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šè‡ªå®šä¹‰â€œä¸¥æ ¼æ¨¡å¼â€åŒ…è£…å™¨
# ==========================================
class StrictSafetyWrapper(gym.Wrapper):
    """
    è¿™æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æ•™ç»ƒï¼š
    1. å¦‚æœç”±äºç¦»ç®¡å­å¤ªè¿‘è€Œè¿‡å…³ï¼Œæ²¡æ”¶å¤§éƒ¨åˆ†å¥–åŠ±ã€‚
    2. æ¯ä¸€å¸§å¦‚æœç¦»éšœç¢ç‰©å¤ªè¿‘ï¼Œç»™äºˆå¾®å°æƒ©ç½šã€‚
    """

    def __init__(self, env, safe_dist=0.25):
        super().__init__(env)
        self.safe_dist = safe_dist  # å®‰å…¨è·ç¦»é˜ˆå€¼ï¼ˆå½’ä¸€åŒ–å€¼ï¼Œ0.0~1.0ï¼‰

    def step(self, action):
        # è·å–åŸå§‹ç¯å¢ƒè¿”å›çš„è§‚æµ‹ã€å¥–åŠ±ã€ç»ˆæ­¢çŠ¶æ€ç­‰
        # obs åœ¨ use_lidar=True æ—¶ï¼Œæ˜¯ä¸€ä¸ªåŒ…å« 180 ä¸ªé›·è¾¾æ•°æ®çš„æ•°ç»„
        # æ•°å€¼è¶Šå°ï¼Œä»£è¡¨ç¦»éšœç¢ç‰©è¶Šè¿‘ï¼ˆ0.0 è¡¨ç¤ºç¢°æ’ï¼Œ1.0 è¡¨ç¤ºå¾ˆè¿œï¼‰
        obs, reward, terminated, truncated, info = self.env.step(action)

        # === ğŸ˜ˆ é­”æ”¹å¥–åŠ±é€»è¾‘ ===

        # 1. è·å–å½“å‰æœ€è¿‘éšœç¢ç‰©çš„è·ç¦»ï¼ˆæ‰€æœ‰é›·è¾¾æ–¹å‘ä¸­çš„æœ€å°å€¼ï¼‰
        min_distance = np.min(obs)  # å¯èƒ½æ¥è‡ªä¸Šæ–¹ï¼ˆä¸Šç®¡ï¼‰ã€ä¸‹æ–¹ï¼ˆä¸‹ç®¡ï¼‰æˆ–å‰æ–¹

        # 2. æƒ©ç½šâ€œè´´è„¸é£è¡Œâ€ (Proximity Penalty)
        # å¦‚æœç¦»ä»»ä½•éšœç¢ç‰©å¤ªè¿‘ï¼ˆå°äºå®‰å…¨é˜ˆå€¼ï¼‰ï¼Œæ¯å¸§æ‰£ä¸€ç‚¹åˆ†
        # ç›®çš„æ˜¯é¼“åŠ±æ™ºèƒ½ä½“ä¿æŒåœ¨ç©ºæ—·åŒºåŸŸï¼Œé¿å…æ“¦è¾¹é£è¡Œ
        if min_distance < self.safe_dist:
            reward -= 0.03  # å¾®å°æƒ©ç½šï¼Œé¿å…æ™ºèƒ½ä½“å› æƒ©ç½šè¿‡é‡è€Œâ€œä¸»åŠ¨è‡ªæ€â€

        # 3. æƒ©ç½šâ€œæƒŠé™©è¿‡å…³â€
        # é»˜è®¤ç¯å¢ƒä¸­ï¼ŒæˆåŠŸç©¿è¿‡ä¸€ä¸ªç®¡é“ä¼šè·å¾— reward >= 1.0
        # å¦‚æœæ­¤æ—¶ç¦»éšœç¢ç‰©ä»å¤ªè¿‘ï¼ˆmin_distance < safe_distï¼‰ï¼Œè¯´æ˜æ˜¯â€œå±é™©é€šå…³â€
        if reward >= 1.0:
            if min_distance < self.safe_dist:
                # è™½ç„¶è¿‡äº†ç®¡å­ï¼Œä½†å¤ªå±é™©ï¼åªç»™å°‘é‡å¥–åŠ±ï¼ˆ0.2 åˆ†ï¼‰
                reward -= 0.4
                # æ³¨ï¼šä¹Ÿå¯ç”¨ reward -= 0.8ï¼Œæ•ˆæœç±»ä¼¼

        # è¿”å›ä¿®æ”¹åçš„ç»“æœ
        return obs, reward, terminated, truncated, info


# ==========================================
# ğŸ› ï¸ ç¯å¢ƒæ„å»ºå‡½æ•°ï¼šç”¨äºåˆ›å»ºå•ä¸ªè®­ç»ƒç¯å¢ƒ
# ==========================================
def make_env():
    # 1. åˆ›å»ºåŸºç¡€ Flappy Bird ç¯å¢ƒï¼Œä½¿ç”¨ LiDAR è§‚æµ‹ï¼ˆ180 ç»´å‘é‡ï¼‰
    env = gym.make("FlappyBird-v0", render_mode=None, use_lidar=True)

    # 2. ğŸ”¥ å¥—ä¸Šæˆ‘ä»¬çš„ä¸¥æ ¼æ•™ç»ƒåŒ…è£…å™¨ï¼Œè®¾ç½®å®‰å…¨è·ç¦»ä¸º 0.2ï¼ˆå³ 20% çš„æ¢æµ‹èŒƒå›´ï¼‰
    env = StrictSafetyWrapper(env, safe_dist=0.2)

    return env


# ==========================================
# ğŸ‹ï¸â€â™‚ï¸ è®­ç»ƒä¸»å‡½æ•°ï¼šæ”¯æŒæ–­ç‚¹ç»­è®­ + è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
# ==========================================
def train():
    print(f"\n>>> [ä¸¥æ ¼æ¨¡å¼] å¯åŠ¨è®­ç»ƒï¼Œå¦‚æœé£å¾—å¤ªè´´è¿‘ç®¡å­ä¼šè¢«æ‰£åˆ†ï¼")
    print(f">>> ç›®æ ‡æ­¥æ•°: {TOTAL_TIMESTEPS}")

    # åˆ›å»º N_ENVS ä¸ªå¹¶è¡Œç¯å¢ƒï¼Œç”¨äºé«˜æ•ˆé‡‡æ ·
    env = make_vec_env(make_env, n_envs=N_ENVS, monitor_dir=LOG_DIR)
    # åˆ›å»ºå•ç‹¬çš„è¯„ä¼°ç¯å¢ƒï¼ˆä¸å‚ä¸è®­ç»ƒï¼‰
    eval_env = make_vec_env(make_env, n_envs=1)

    # å°è¯•åŠ è½½å†å²æœ€ä½³æ¨¡å‹ï¼Œç”¨äºåˆå§‹åŒ– best_mean_reward
    best_model_path = os.path.join(MODELS_DIR, f"{BEST_MODEL_NAME}.zip")
    historical_best_score = -np.inf  # åˆå§‹åŒ–ä¸ºè´Ÿæ— ç©·

    if os.path.exists(best_model_path):
        print(f">>> ğŸ† å‘ç°å†å²æ¨¡å‹ï¼Œæ­£åœ¨è¯„ä¼°...")
        try:
            temp_model = PPO.load(best_model_path)
            # ç”¨ 5 å±€è¯„ä¼°å¹³å‡å¾—åˆ†ä½œä¸ºå†å²æœ€ä½³
            mean_reward, _ = evaluate_policy(temp_model, eval_env, n_eval_episodes=5)
            historical_best_score = mean_reward
            print(f">>> ğŸ“Š å†å²æœ€é«˜åˆ†: {historical_best_score:.2f}")
            del temp_model  # é‡Šæ”¾å†…å­˜
        except Exception as e:
            print(f">>> âš ï¸ åŠ è½½å†å²æ¨¡å‹å¤±è´¥: {e}")
            pass

    # å†³å®šæ˜¯æ–°å»ºæ¨¡å‹è¿˜æ˜¯ç»§ç»­è®­ç»ƒ
    final_model_path = os.path.join(MODELS_DIR, f"{FINAL_MODEL_NAME}.zip")
    if os.path.exists(final_model_path):
        print(">>> â™»ï¸ ç»§ç»­è®­ç»ƒ...")
        model = PPO.load(final_model_path, env=env)  # åŠ è½½å¹¶ç»‘å®šæ–°ç¯å¢ƒ
    else:
        print(">>> âœ¨ æ–°å»ºæ¨¡å‹...")
        # å®šä¹‰ PPO æ¨¡å‹ç»“æ„å’Œè¶…å‚æ•°
        model = PPO(
            "MlpPolicy",               # ä½¿ç”¨å…¨è¿æ¥ç½‘ç»œï¼ˆMLPï¼‰
            env,
            verbose=1,                 # æ‰“å°è®­ç»ƒæ—¥å¿—
            tensorboard_log=LOG_DIR,   # TensorBoard æ—¥å¿—è·¯å¾„
            learning_rate=3e-4,        # å­¦ä¹ ç‡
            n_steps=2048,              # æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•°
            batch_size=64,             # è®­ç»ƒæ‰¹æ¬¡å¤§å°
            n_epochs=10,               # æ¯æ‰¹æ•°æ®è®­ç»ƒè½®æ•°
            gamma=0.99,                # æŠ˜æ‰£å› å­
            gae_lambda=0.95,           # GAE å‚æ•°
            clip_range=0.2,            # PPO çš„è£å‰ªèŒƒå›´
            ent_coef=0.01,             # ç†µæ­£åˆ™åŒ–ç³»æ•°ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
            policy_kwargs=dict(
                net_arch=dict(pi=[128, 128], vf=[128, 128]),  # ç­–ç•¥å’Œä»·å€¼ç½‘ç»œç»“æ„
                activation_fn=torch.nn.Tanh  # æ¿€æ´»å‡½æ•°
            ),
        )

    # è®¾ç½®è¯„ä¼°å›è°ƒï¼šæ¯ 10000 æ­¥è¯„ä¼°ä¸€æ¬¡ï¼Œè‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=MODELS_DIR,
        log_path=LOG_DIR,
        eval_freq=10000 // N_ENVS,     # æ³¨æ„ï¼ševal_freq æ˜¯æŒ‰æ¯ä¸ªç¯å¢ƒçš„æ­¥æ•°è®¡ç®—
        n_eval_episodes=5,             # æ¯æ¬¡è¯„ä¼°è·‘ 5 å±€
        deterministic=True,            # ä½¿ç”¨ç¡®å®šæ€§åŠ¨ä½œï¼ˆå…³é—­æ¢ç´¢ï¼‰
        render=False                   # ä¸æ¸²æŸ“è¯„ä¼°è¿‡ç¨‹
    )
    # æ‰‹åŠ¨è®¾ç½®å†å²æœ€ä½³åˆ†æ•°ï¼Œé¿å…è¦†ç›–å·²æœ‰æœ€ä½³æ¨¡å‹
    eval_callback.best_mean_reward = historical_best_score

    # å¼€å§‹è®­ç»ƒ
    try:
        model.learn(
            total_timesteps=TOTAL_TIMESTEPS,
            callback=eval_callback,
            progress_bar=True,          # æ˜¾ç¤ºè¿›åº¦æ¡
            reset_num_timesteps=False   # ç»­è®­æ—¶ä¸é‡ç½® timestep è®¡æ•°
        )
    except KeyboardInterrupt:
        print(">>> ä¸­æ–­ä¿å­˜...")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(final_model_path)
    env.close()
    eval_env.close()


# ==========================================
# ğŸ® æµ‹è¯•å‡½æ•°ï¼šå¯è§†åŒ–è¿è¡Œ + æ­»äº¡æš‚åœåŠŸèƒ½ï¼ˆä¾¿äºåˆ†æå¤±è´¥åŸå› ï¼‰
# ==========================================
import time  # ç”¨äºè®¡ç®—æ¯å±€å­˜æ´»æ—¶é—´


def test():
    # å°è¯•åŠ è½½æœ€ä½³æ¨¡å‹ï¼Œè‹¥æ— åˆ™åŠ è½½æœ€ç»ˆæ¨¡å‹
    load_path = os.path.join(MODELS_DIR, f"{BEST_MODEL_NAME}.zip")
    if not os.path.exists(load_path):
        load_path = os.path.join(MODELS_DIR, f"{FINAL_MODEL_NAME}.zip")

    if not os.path.exists(load_path):
        print("âŒ æ— æ¨¡å‹")
        return

    print(f">>> ğŸ® æ­£åœ¨åŠ è½½æ¨¡å‹: {load_path}")
    print(f">>> ğŸ•µï¸ æ­»äº¡æš‚åœæ¨¡å¼å·²å¼€å¯ï¼šæ’æ­»åè¯·çœ‹ç”»é¢ï¼ŒæŒ‰å›è½¦ç»§ç»­ï¼")

    # åˆ›å»ºå¯æ¸²æŸ“çš„äººç±»å¯è§†åŒ–ç¯å¢ƒï¼ˆrender_mode="human"ï¼‰
    env = gym.make("FlappyBird-v0", render_mode="human", use_lidar=True)
    model = PPO.load(load_path)

    # è¿è¡Œ 10 å±€æµ‹è¯•
    for ep in range(10):
        obs, _ = env.reset()  # é‡ç½®ç¯å¢ƒ
        done = False
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

        print(f"\nğŸ¬ ç¬¬ {ep + 1} å±€å¼€å§‹...")

        while not done:
            # ä½¿ç”¨æ¨¡å‹é¢„æµ‹åŠ¨ä½œï¼ˆç¡®å®šæ€§ç­–ç•¥ï¼‰
            action, _ = model.predict(obs, deterministic=True)
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = env.step(action)

            # åªæœ‰æ’åˆ°éšœç¢ç‰©æ‰ç®—çœŸæ­£ç»“æŸï¼ˆå¿½ç•¥æ—¶é—´æˆªæ–­ï¼‰
            done = terminated

            # å¦‚æœç¯å¢ƒå› æœ€å¤§æ­¥æ•°æˆªæ–­ï¼ˆtruncatedï¼‰ï¼Œæˆ‘ä»¬å¿½ç•¥å®ƒï¼Œç»§ç»­é£
            if truncated:
                pass

            # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒåŠŸèƒ½ï¼šæ­»äº¡åæš‚åœï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤
            if terminated:
                duration = time.time() - start_time  # è®¡ç®—å­˜æ´»æ—¶é—´
                final_score = info.get('score', 0)   # è·å–æœ€ç»ˆå¾—åˆ†ï¼ˆé€šè¿‡çš„ç®¡é“æ•°ï¼‰

                print(f"ğŸ›‘ [æ’è½¦ç¬é—´]ï¼")
                print(f"   åˆ†æ•°: {final_score} | å­˜æ´»: {duration:.2f}ç§’")
                print(f"   ğŸ‘€ è¯·æ£€æŸ¥æ¸¸æˆçª—å£ï¼Œçœ‹çœ‹åˆ°åº•æ’åˆ°äº†å“ªé‡Œï¼ˆé¡¶éƒ¨ï¼Ÿåº•éƒ¨ï¼Ÿç®¡å­è¾¹ç¼˜ï¼Ÿï¼‰")

                # â¸ï¸ ç¨‹åºæš‚åœï¼Œç­‰å¾…ç”¨æˆ·æŒ‰å›è½¦é”®ç»§ç»­ä¸‹ä¸€å±€
                input("ğŸ‘‰ æŒ‰ [å›è½¦é”® Enter] å¼€å§‹ä¸‹ä¸€å±€...")

    # å…³é—­ç¯å¢ƒ
    env.close()


# ==========================================
# ä¸»ç¨‹åºå…¥å£
# ==========================================
if __name__ == "__main__":
    # train()  # å¦‚éœ€è®­ç»ƒï¼Œå–æ¶ˆæ³¨é‡Šæ­¤è¡Œ
    test()     # å½“å‰é»˜è®¤è¿è¡Œæµ‹è¯•