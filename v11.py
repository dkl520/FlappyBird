import os
import gymnasium as gym
import flappy_bird_gymnasium
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.vec_env import SubprocVecEnv, VecFrameStack, DummyVecEnv
import torch

# ================= ğŸš€ é…ç½®åŒºåŸŸ =================
ENV_ID = "FlappyBird-v0"
MODEL_DIR = "trained_models"
LOG_DIR = "logs"
MODEL_NAME = "FlappyBird_Master"
TOTAL_TIMESTEPS = 2_000_000  # å»ºè®®ç”±200ä¸‡æ­¥èµ·ç»ƒ
STACK_NUM = 4  # æ ¸å¿ƒï¼šå †å  4 å¸§

os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


def make_env(render_mode=None):
    def _init():
        env = gym.make(
            ENV_ID,
            render_mode=render_mode,
            use_lidar=True,
            background=None
        )
        return env

    return _init


def train():
    print("ğŸš€ å¼€å§‹è®­ç»ƒ (Frame Stacking ç‰ˆ)...")
    print("ğŸ’¡ æç¤ºï¼šå¦‚æœæ²¡çœ‹åˆ°ç»¿è‰²è¿›åº¦æ¡ï¼Œè¯·ç¡®ä¿è¿è¡Œäº†: pip install tqdm rich")

    num_cpu = 4
    # 1. åˆ›å»ºåŸºç¡€ç¯å¢ƒ
    env = SubprocVecEnv([make_env() for _ in range(num_cpu)])

    # 2. æŠŠç¯å¢ƒåŒ…åœ¨ VecFrameStack é‡Œ
    env = VecFrameStack(env, n_stack=STACK_NUM)

    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        learning_rate=2.5e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        ent_coef=0.01,
        tensorboard_log=LOG_DIR,
        device="cpu"  # <---ã€ä¿®æ”¹ç‚¹1ã€‘å¼ºåˆ¶ä½¿ç”¨ CPUï¼Œæ¶ˆé™¤è­¦å‘Šä¸”é€Ÿåº¦æ›´å¿«
    )

    checkpoint_callback = CheckpointCallback(
        save_freq=100_000 // num_cpu,
        save_path=MODEL_DIR,
        name_prefix="ppo_stacked"
    )

    # <---ã€ä¿®æ”¹ç‚¹2ã€‘progress_bar=True å·²å¯ç”¨
    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=checkpoint_callback, progress_bar=True)

    final_path = os.path.join(MODEL_DIR, MODEL_NAME)
    model.save(final_path)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {final_path}")
    env.close()


def test():
    print("ğŸ‘€ æ­£åœ¨åŠ è½½ Frame Stacking æ¨¡å‹...")
    model_path = os.path.join(MODEL_DIR, MODEL_NAME + ".zip")

    if not os.path.exists(model_path):
        print(f"âŒ æ‰¾ä¸åˆ° {model_path}")
        return

    # æµ‹è¯•ç¯å¢ƒä¹Ÿè¦åšåŒæ ·çš„åŒ…è£¹
    env = DummyVecEnv([make_env(render_mode="human")])
    env = VecFrameStack(env, n_stack=STACK_NUM)  # æµ‹è¯•æ—¶å¿…é¡»å †å 

    # åŠ è½½æ¨¡å‹
    model = PPO.load(model_path, env=env, device="cpu")  # æµ‹è¯•æ—¶ä¹Ÿå»ºè®®æŒ‡å®š CPU

    episodes = 5
    for ep in range(episodes):
        obs = env.reset()
        done = False
        total_reward = 0

        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            env.render()

            total_reward += reward[0]

            if done[0]:
                print(f"ğŸ Episode {ep + 1} ç»“æŸï¼Œå¾—åˆ†: {info[0].get('score', 'unknown')}")
                break

    env.close()


if __name__ == "__main__":
    # train()  # å…ˆè·‘ Train
    test()  # å†è·‘ Test