import gymnasium as gym
import flappy_bird_gymnasium
import torch
import os
import numpy as np
from stable_baselines3 import PPO  # ğŸ”¥ æ ¸å¿ƒæ”¹å˜ï¼šä½¿ç”¨ PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor

# ==========================================
# âš™ï¸ æ ¸å¿ƒé…ç½® (State of the Art)
# ==========================================
MODELS_DIR = "models/flappy_ppo_v3"
LOG_DIR = "logs/flappy_ppo_v3"
MODEL_NAME = "flappy_bird_ppo_best"

# âš¡ å¹¶è¡Œç¯å¢ƒæ•°ï¼šCPUæ ¸å¿ƒæ•°è¶Šå¤šè¶Šå¥½ï¼Œé€šå¸¸è®¾ç½® 4-8
N_ENVS = 4

os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


# ==========================================
# ğŸš€ è®­ç»ƒç¯å¢ƒæ„å»º
# ==========================================
def make_env():
    # ğŸ”¥ å…³é”®ç§˜ç±ï¼šuse_lidar=True
    # è¿™ä¼šç»™ AI æä¾› 180 ä¸ªè·ç¦»ä¼ æ„Ÿå™¨æ•°æ®ï¼Œç›¸æ¯”åªæœ‰åæ ‡ï¼Œè¿™ç®€ç›´æ˜¯å¼€äº†æŒ‚
    env = gym.make("FlappyBird-v0", render_mode=None, use_lidar=True)
    return env


def train():
    print(f">>> [åˆå§‹åŒ–] å¯åŠ¨ {N_ENVS} ä¸ªå¹¶è¡Œç¯å¢ƒ (PPOç®—æ³•)...")

    # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
    env = make_vec_env(make_env, n_envs=N_ENVS, monitor_dir=LOG_DIR)

    # PPO è¶…å‚æ•° (é’ˆå¯¹ Flappy Bird è°ƒä¼˜)
    # ç›¸æ¯” DQNï¼ŒPPO å¯¹è¶…å‚æ•°ä¸é‚£ä¹ˆæ•æ„Ÿï¼Œæ›´å®¹æ˜“è®­ç»ƒ
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        tensorboard_log=LOG_DIR,
        learning_rate=3e-4,  # æ ‡å‡†å­¦ä¹ ç‡
        n_steps=2048,  # æ¯æ¬¡æ›´æ–°çš„é‡‡æ ·æ­¥æ•°
        batch_size=64,  # å°æ‰¹é‡å¤§å°
        n_epochs=10,  # æ¯æ¬¡æ›´æ–°çš„è¿­ä»£æ¬¡æ•°
        gamma=0.99,  # æŠ˜æ‰£å› å­
        gae_lambda=0.95,  # ä¼˜åŠ¿ä¼°è®¡å‚æ•°
        clip_range=0.2,  # PPO è£å‰ªèŒƒå›´
        ent_coef=0.01,  # ğŸ”¥ ç†µç³»æ•°ï¼šå¼ºåˆ¶ AI å°è¯•ä¸åŒåŠ¨ä½œï¼Œé˜²æ­¢æ—©ç†Ÿ
        policy_kwargs=dict(
            net_arch=dict(pi=[128, 128], vf=[128, 128]),  # ç½‘ç»œç»“æ„
            activation_fn=torch.nn.Tanh
        ),
    )

    # æ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„æ¨¡å‹ç»§ç»­è®­ç»ƒ
    final_path = f"{MODELS_DIR}/{MODEL_NAME}.zip"
    if os.path.exists(final_path):
        print(f">>> â™»ï¸ æ£€æµ‹åˆ°æ—§æ¨¡å‹ï¼Œæ­£åœ¨åŠ è½½ç»§ç»­è®­ç»ƒ...")
        model = PPO.load(final_path, env=env)

    # å›è°ƒå‡½æ•°ï¼šä¿å­˜æœ€ä¼˜æ¨¡å‹
    eval_env = make_vec_env(make_env, n_envs=1)
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=MODELS_DIR,
        log_path=LOG_DIR,
        eval_freq=10000,
        n_eval_episodes=5,
        deterministic=True,
        render=False
    )

    # è®­ç»ƒæ­¥æ•°ï¼šPPO æ•ˆç‡å¾ˆé«˜ï¼Œ50ä¸‡æ­¥é€šå¸¸å°±èƒ½è¾¾åˆ°â€œä¸æ­»â€çŠ¶æ€
    # ä¹Ÿå°±æ˜¯ç°å®æ—¶é—´å¤§çº¦ 5-10 åˆ†é’Ÿ
    TOTAL_STEPS = 1_000_000
    print(f">>> [å¼€å§‹] ç›®æ ‡è®­ç»ƒæ­¥æ•°: {TOTAL_STEPS}...")

    try:
        model.learn(
            total_timesteps=TOTAL_STEPS,
            callback=eval_callback,
            progress_bar=True
        )
        model.save(f"{MODELS_DIR}/{MODEL_NAME}")
        print(">>> âœ… è®­ç»ƒå®Œæˆï¼")
    except KeyboardInterrupt:
        print(">>> ğŸ›‘ è®­ç»ƒä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰æ¨¡å‹...")
        model.save(f"{MODELS_DIR}/interrupted_ppo")

    env.close()


# ==========================================
# ğŸ§ª æµ‹è¯•å‡½æ•° (äº«å—æˆæœ)
# ==========================================
def test():
    model_path = f"{MODELS_DIR}/best_model.zip"
    if not os.path.exists(model_path):
        model_path = f"{MODELS_DIR}/{MODEL_NAME}.zip"
        if not os.path.exists(model_path):
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼è¯·å…ˆè¿è¡Œè®­ç»ƒã€‚")
            return

    print(f">>> ğŸ® æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")

    # æµ‹è¯•æ—¶å¼€å¯ render_mode="human" è§‚çœ‹ AI æ“ä½œ
    # è®°å¾—ä¹Ÿè¦å¼€å¯ use_lidar=Trueï¼Œå¦åˆ™è¾“å…¥ç»´åº¦å¯¹ä¸ä¸Š
    env = gym.make("FlappyBird-v0", render_mode="human", use_lidar=True)

    model = PPO.load(model_path)

    episodes = 10
    for ep in range(episodes):
        obs, _ = env.reset()
        done = False
        score = 0
        while not done:
            # deterministic=True è®© AI å‘æŒ¥ç¨³å®šå®åŠ›ï¼Œä¸è¿›è¡Œéšæœºæ¢ç´¢
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            score = info.get('score', score)

        print(f"ç¬¬ {ep + 1} å±€å¾—åˆ†: {score}")

    env.close()


if __name__ == "__main__":
    # 1. å…ˆè¿è¡Œè®­ç»ƒ (åªéœ€è¿è¡Œä¸€æ¬¡ï¼Œå¤§çº¦ 5-10 åˆ†é’Ÿ)
    # train()

    # 2. è®­ç»ƒå®Œæˆåæ³¨é‡Šæ‰ train()ï¼Œå–æ¶ˆä¸‹é¢ test() çš„æ³¨é‡Šæ¥è§‚çœ‹
    test()