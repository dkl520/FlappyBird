import os
import numpy as np
import gymnasium as gym
import flappy_bird_gymnasium
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
from tqdm import tqdm  # å¯¼å…¥è¿›åº¦æ¡åº“

# ================= ğŸš€ è¶…å‚æ•°é…ç½® =================
ENV_ID = "FlappyBird-v0"
LEARNING_RATE = 2.5e-4
GAMMA = 0.99  # æŠ˜æ‰£å› å­
LAMBDA = 0.95  # GAE å‚æ•°
EPS_CLIP = 0.2  # PPO æˆªæ–­èŒƒå›´
K_EPOCHS = 10  # æ›´æ–°å¾ªç¯æ¬¡æ•°
BATCH_SIZE = 64  # å°æ‰¹é‡å¤§å°
UPDATE_TIMESTEP = 2048  # æ”¶é›†å¤šå°‘æ­¥æ•°æ®æ›´æ–°ä¸€æ¬¡
TOTAL_TIMESTEPS = 1_000_000
ENTROPY_COEF = 0.01

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = "cpu"  # ä¿æŒä½ åŸæ¥çš„è®¾ç½®
print(f"ğŸ“Œ ä½¿ç”¨è®¾å¤‡: {device}")


# ================= ğŸ›¡ï¸ å®‰å…¨å¥–åŠ±åŒ…è£…å™¨ =================
class StrictSafetyWrapper(gym.Wrapper):
    def __init__(self, env, safe_dist=0.10):
        super().__init__(env)
        self.safe_dist = safe_dist

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        min_dist = np.min(obs)

        # æƒ©ç½šè´´ç®¡é£è¡Œ (è´Ÿåé¦ˆ)
        if min_dist < self.safe_dist:
            reward -= 0.03

        return obs, reward, terminated, truncated, info


# ================= ğŸ§  1. å®šä¹‰ Actor-Critic ç½‘ç»œ =================
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(ActorCritic, self).__init__()

        # å…±äº«ç‰¹å¾æå–å±‚
        self.base_layer = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.Tanh(),
            nn.Linear(128, 128),
            nn.Tanh()
        )

        # Actor
        self.actor = nn.Sequential(
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )

        # Critic
        self.critic = nn.Linear(128, 1)

    def act(self, state):
        x = self.base_layer(state)
        action_probs = self.actor(x)
        dist = Categorical(action_probs)

        action = dist.sample()
        action_logprob = dist.log_prob(action)
        state_val = self.critic(x)

        return action.item(), action_logprob.item(), state_val.item()

    def evaluate(self, state, action):
        x = self.base_layer(state)
        action_probs = self.actor(x)
        dist = Categorical(action_probs)

        action_logprobs = dist.log_prob(action)
        dist_entropy = dist.entropy()
        state_values = self.critic(x)

        return action_logprobs, state_values, dist_entropy


# ================= ğŸ› ï¸ 2. PPO ç®—æ³•é€»è¾‘ =================
class PPO:
    def __init__(self, state_dim, action_dim):
        self.policy = ActorCritic(state_dim, action_dim).to(device)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=LEARNING_RATE)
        self.policy_old = ActorCritic(state_dim, action_dim).to(device)
        self.policy_old.load_state_dict(self.policy.state_dict())
        self.MseLoss = nn.MSELoss()

    def update(self, memory):
        old_states = torch.stack(memory.states).detach().to(device)
        old_actions = torch.stack(memory.actions).detach().to(device)
        old_logprobs = torch.stack(memory.logprobs).detach().to(device)
        old_state_values = torch.stack(memory.state_values).detach().to(device).squeeze()

        rewards = memory.rewards
        is_terminals = memory.is_terminals
        advantages = []
        gae = 0

        # GAE è®¡ç®—
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_val = 0
            else:
                next_val = old_state_values[i + 1].item()
            curr_val = old_state_values[i].item()
            mask = 1 - is_terminals[i]
            delta = rewards[i] + GAMMA * next_val * mask - curr_val
            gae = delta + GAMMA * LAMBDA * mask * gae
            advantages.insert(0, gae)

        advantages = torch.tensor(advantages, dtype=torch.float32).to(device)
        returns = advantages + old_state_values
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-7)

        dataset_size = old_states.size(0)

        # PPO æ›´æ–°
        for _ in range(K_EPOCHS):
            for index in range(0, dataset_size, BATCH_SIZE):
                batch_indices = slice(index, min(index + BATCH_SIZE, dataset_size))
                batch_states = old_states[batch_indices]
                batch_actions = old_actions[batch_indices]
                batch_logprobs = old_logprobs[batch_indices]
                batch_advantages = advantages[batch_indices]
                batch_returns = returns[batch_indices]

                logprobs, state_values, dist_entropy = self.policy.evaluate(batch_states, batch_actions)
                state_values = torch.squeeze(state_values)
                ratio = torch.exp(logprobs - batch_logprobs)

                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - EPS_CLIP, 1 + EPS_CLIP) * batch_advantages
                loss_actor = -torch.min(surr1, surr2).mean()
                loss_critic = self.MseLoss(state_values, batch_returns)
                loss = loss_actor + 0.5 * loss_critic - ENTROPY_COEF * dist_entropy.mean()

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

        self.policy_old.load_state_dict(self.policy.state_dict())

    def save(self, checkpoint_path):
        torch.save(self.policy_old.state_dict(), checkpoint_path)

    def load(self, checkpoint_path):
        # åŠ ä¸Š weights_only=True (å¦‚æœä½ çš„pytorchç‰ˆæœ¬è¾ƒæ–°) æˆ–è€…å¿½ç•¥å®ƒ
        # è¿™é‡Œä¸ºäº†å…¼å®¹æ€§ï¼Œé€šå¸¸å¯ä»¥ä¿æŒåŸæ ·ï¼Œæˆ–è€…æ˜¾å¼åŠ ä¸Š weights_only=False æ¶ˆé™¤æ­§ä¹‰
        state_dict = torch.load(checkpoint_path, map_location=device, weights_only=False)
        self.policy_old.load_state_dict(state_dict)
        self.policy.load_state_dict(state_dict)

class Memory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []
        self.state_values = []

    def clear(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]
        del self.state_values[:]


# ================= ğŸƒ è®­ç»ƒæµç¨‹ (å¸¦æ–­ç‚¹ç»­è®­ + å½©è‰²è¿›åº¦æ¡) =================
def train():
    print("ğŸš€ å‡†å¤‡å¼€å§‹è®­ç»ƒ...")

    env = gym.make(ENV_ID, use_lidar=True, background=None)
    env = StrictSafetyWrapper(env, safe_dist=0.09)

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    memory = Memory()
    ppo_agent = PPO(state_dim, action_dim)

    # ------------------- ğŸ”„ æ–­ç‚¹ç»­è®­é€»è¾‘ -------------------
    model_dir = "manual_models"
    final_model_name = "ppo_flappy_final.pth"
    resume_path = os.path.join(model_dir, final_model_name)

    if os.path.exists(resume_path):
        print(f"ğŸ”„ å‘ç°ä¸Šæ¬¡è®­ç»ƒæ¨¡å‹: {resume_path}")
        try:
            ppo_agent.load(resume_path)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å°†åœ¨è¯¥æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒ (Resume Training)")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½å‡ºé”™ ({e})ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
    else:
        print("ğŸ†• æœªæ‰¾åˆ°å·²æœ‰æ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ (Start From Scratch)")
    # ------------------------------------------------------

    time_step = 0
    running_reward = 0  # ç”¨äºè®¡ç®—å¹³æ»‘å¹³å‡åˆ†

    # ğŸŸ¢ åˆå§‹åŒ– tqdm è¿›åº¦æ¡ (colour='green' å®ç°å½©è‰²æ•ˆæœ)
    pbar = tqdm(total=TOTAL_TIMESTEPS, desc="Training", unit="step", colour='green')

    while time_step < TOTAL_TIMESTEPS:
        state, _ = env.reset()
        current_ep_reward = 0
        done = False

        while not done:
            state_tensor = torch.FloatTensor(state).to(device)
            action, logprob, val = ppo_agent.policy_old.act(state_tensor)

            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

            memory.states.append(state_tensor)
            memory.actions.append(torch.tensor(action).to(device))
            memory.logprobs.append(torch.tensor(logprob).to(device))
            memory.state_values.append(torch.tensor(val).to(device))
            memory.rewards.append(reward)
            memory.is_terminals.append(terminated)

            state = next_state
            current_ep_reward += reward
            time_step += 1

            # ğŸŸ¢ æ›´æ–°è¿›åº¦æ¡
            pbar.update(1)

            # PPO æ›´æ–°
            if time_step % UPDATE_TIMESTEP == 0:
                ppo_agent.update(memory)
                memory.clear()

            if done:
                break

        # ğŸŸ¢ è®¡ç®—å¹³æ»‘å¹³å‡åˆ†
        if running_reward == 0:
            running_reward = current_ep_reward
        else:
            running_reward = 0.05 * current_ep_reward + 0.95 * running_reward

        # ğŸŸ¢ è®¾ç½®è¿›åº¦æ¡åç¼€
        pbar.set_postfix({
            'Last': f'{current_ep_reward:.2f}',
            'Avg': f'{running_reward:.2f}'
        })

        # ================= ğŸ’¾ å®šæœŸä¿å­˜ =================
        if time_step % 50000 == 0:
            os.makedirs(model_dir, exist_ok=True)  # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
            save_path = os.path.join(model_dir, f"ppo_flappy_{time_step}.pth")

            ppo_agent.save(save_path)
            abs_path = os.path.abspath(save_path)
            pbar.write(f"ğŸ’¾ é˜¶æ®µä¿å­˜: {abs_path}")

    pbar.close()  # å…³é—­è¿›åº¦æ¡

    # ================= ğŸ’¾ æœ€ç»ˆä¿å­˜ =================
    try:
        os.makedirs(model_dir, exist_ok=True)  # å†æ¬¡ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
        final_save_path = os.path.join(model_dir, final_model_name)

        ppo_agent.save(final_save_path)

        print("âœ… è®­ç»ƒç»“æŸ")
        print(f"ğŸ“ æœ€ç»ˆæ¨¡å‹ä½ç½®: {os.path.abspath(final_save_path)}")

    except Exception as e:
        print(f"âŒ æœ€ç»ˆä¿å­˜å¤±è´¥: {e}")

    env.close()

def test():
    print("ğŸ‘€ åŠ è½½æ¨¡å‹è¿›è¡Œæµ‹è¯• (æ— å°½æ¨¡å¼)...")
    print("ğŸ’¡ æç¤ºï¼šæŒ‰ Ctrl+C å¯ä»¥å¼ºåˆ¶åœæ­¢ç¨‹åº")

    # 1. è®¾ç½®æå¤§çš„æ­¥æ•°é™åˆ¶ (1äº¿æ­¥)ï¼Œç¡®ä¿ä¸ä¼šå› ä¸ºè¶…æ—¶è€Œé‡ç½®
    env = gym.make(ENV_ID, render_mode="human", use_lidar=True, background=None, max_episode_steps=100000000)

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    ppo_agent = PPO(state_dim, action_dim)
    model_path = "manual_models/ppo_flappy_final.pth"

    if not os.path.exists(model_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹: {model_path}ï¼Œè¯·å…ˆè¿è¡Œ train() è¿›è¡Œè®­ç»ƒã€‚")
        return

    ppo_agent.load(model_path)

    episode_cnt = 0

    # 2. æ”¹å› while Trueï¼Œå®ç°çœŸæ­£çš„â€œæ— é™å±€æ•°â€
    while True:
        episode_cnt += 1
        state, _ = env.reset()
        terminated = False
        truncated = False  # åˆå§‹åŒ– truncated
        score = 0
        step_cnt = 0

        # åªè¦æ²¡æ­»ï¼Œå°±ä¸€ç›´é£ (å¿½ç•¥ truncatedï¼Œé™¤éä½ çœŸçš„æƒ³çœ‹å®ƒé£ä¸€äº¿æ­¥)
        while not terminated:
            step_cnt += 1
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)

            with torch.no_grad():
                features = ppo_agent.policy.base_layer(state_tensor)
                action_probs = ppo_agent.policy.actor(features)
                action = torch.argmax(action_probs).item()

            state, reward, terminated, truncated, info = env.step(action)
            score = info.get('score', 0)

            # å¦‚æœè§¦å‘äº† truncated (è™½ç„¶è®¾ç½®äº†1äº¿æ­¥ä¸å¤ªå¯èƒ½)ï¼Œæˆ‘ä»¬å¼ºè¡Œè®©å®ƒä¸è¦åœ
            # æ³¨æ„ï¼šå¦‚æœç¯å¢ƒå†…éƒ¨æœ‰ç¡¬æ€§æ—¶é—´é™åˆ¶ï¼Œè¿™é‡Œå¯èƒ½ä¼šå‡ºè­¦å‘Šï¼Œä½†é€šå¸¸æœ‰æ•ˆ
            if truncated:
                # æ‰“å°ä¸€ä¸‹çœ‹çœ‹æ˜¯ä¸æ˜¯çœŸçš„è¶…æ—¶äº†
                print("è¶…æ—¶äº†ï¼ï¼ï¼ï¼ï¼")
                pass

                # ğŸ›‘ æ¸¸æˆç»“æŸï¼Œæ‰“å°åŸå› 
        print(f"Episode {episode_cnt} | Score: {score} | Steps: {step_cnt}")
        if terminated:
            print(f"   ğŸ’€ æ­»äº¡åŸå› : Terminated (åˆ¤å®šæ­»äº¡ï¼Œå¯èƒ½æ˜¯æ’æŸ±å­ã€æ‰åœ°æˆ– **æ’å¤©èŠ±æ¿**)")
        elif truncated:
            print(f"   â³ ç»“æŸåŸå› : Truncated (è¶…æ—¶å¼ºåˆ¶ç»“æŸ)")

        # æš‚åœ 1 ç§’è®©ä½ çœ‹æ¸…æœ€åçš„ç”»é¢
        import time
        time.sleep(10)

    env.close()
if __name__ == "__main__":
    # train()  # è®­ç»ƒæ¨¡å¼
    test()   # æµ‹è¯•æ¨¡å¼