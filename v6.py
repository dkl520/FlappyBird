import gymnasium as gym
import flappy_bird_gymnasium
import torch
import os
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.evaluation import evaluate_policy

# ==========================================
# âš™ï¸ å…¨å±€é…ç½®
# ==========================================
# æ–‡ä»¶å¤¹è·¯å¾„
MODELS_DIR = "models/flappy_ppo_final"
LOG_DIR = "logs/flappy_ppo_final"

# æ–‡ä»¶ååŒºåˆ†
# BEST_MODEL_NAME: æ°¸è¿œåªå­˜å†å²æœ€é«˜åˆ†ï¼ˆå·…å³°çŠ¶æ€ï¼‰
# FINAL_MODEL_NAME: å­˜æœ€åä¸€æ¬¡è®­ç»ƒç»“æŸæ—¶çš„çŠ¶æ€ï¼ˆå“ªæ€•å˜ç¬¨äº†ä¹Ÿå­˜è¿™é‡Œï¼Œç”¨äºç»­è®­ï¼‰
BEST_MODEL_NAME = "best_model"
FINAL_MODEL_NAME = "last_run_model"

# å¹¶è¡Œç¯å¢ƒæ•° (æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´)
N_ENVS = 4
# æ€»è®­ç»ƒæ­¥æ•°
TOTAL_TIMESTEPS = 1_000_000

# åˆ›å»ºç›®å½•
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


# ==========================================
# ğŸ› ï¸ ç¯å¢ƒæ„å»ºå‡½æ•°
# ==========================================
def make_env():
    # use_lidar=True æ˜¯å…³é”®ï¼Œå¼€å¯é›·è¾¾æ¢æµ‹
    env = gym.make("FlappyBird-v0", render_mode=None, use_lidar=True)
    return env


# ==========================================
# ğŸ‹ï¸â€â™‚ï¸ è®­ç»ƒä¸»å‡½æ•°
# ==========================================
def train():
    print(f"\n>>> [ç³»ç»Ÿå¯åŠ¨] å‡†å¤‡å¼€å§‹è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {TOTAL_TIMESTEPS}")

    # 1. åˆ›å»ºå¹¶è¡Œçš„è®­ç»ƒç¯å¢ƒ
    env = make_vec_env(make_env, n_envs=N_ENVS, monitor_dir=LOG_DIR)

    # 2. å‡†å¤‡è¯„ä¼°ç¯å¢ƒ (ç”¨äºæµ‹è¯•å½“å‰æ¨¡å‹å¥½åï¼Œå†³å®šæ˜¯å¦ä¿å­˜ best_model)
    eval_env = make_vec_env(make_env, n_envs=1)

    # ======================================================
    # ğŸ”¥ æ ¸å¿ƒé€»è¾‘ï¼šé˜²æ­¢é‡å¯è®­ç»ƒæ—¶è¦†ç›–æ‰å†å²æœ€é«˜åˆ†
    # ======================================================
    best_model_path = os.path.join(MODELS_DIR, f"{BEST_MODEL_NAME}.zip")
    historical_best_score = -np.inf  # é»˜è®¤è´Ÿæ— ç©·

    if os.path.exists(best_model_path):
        print(f">>> ğŸ† å‘ç°å†å² 'best_model.zip'ï¼Œæ­£åœ¨æµ‹è¯•å®ƒçš„å«é‡‘é‡...")
        try:
            # åŠ è½½æ—§çš„å·…å³°æ¨¡å‹ï¼Œè·‘ 5 å±€çœ‹çœ‹å®ƒåˆ°åº•å¤šå°‘åˆ†
            temp_model = PPO.load(best_model_path)
            mean_reward, _ = evaluate_policy(temp_model, eval_env, n_eval_episodes=5)
            historical_best_score = mean_reward
            print(f">>> ğŸ“Š ç¡®è®¤å†å²æœ€é«˜çºªå½•: {historical_best_score:.2f} åˆ†")
            print("    (åªæœ‰æ–°æ¨¡å‹è¶…è¿‡è¿™ä¸ªåˆ†æ•°ï¼Œæ‰ä¼šè¦†ç›– best_model.zip)")
            del temp_model  # é‡Šæ”¾å†…å­˜
        except Exception as e:
            print(f">>> âš ï¸ å†å²æ¨¡å‹è¯»å–å¤±è´¥ï¼Œå°†é‡æ–°å¼€å§‹è®°å½•ã€‚é”™è¯¯: {e}")
    else:
        print(">>> ğŸ†• æ²¡æœ‰å‘ç°å†å²è®°å½•ï¼Œå°†å»ºç«‹æ–°çš„æ’è¡Œæ¦œã€‚")

    # ======================================================
    # ğŸ§  æ¨¡å‹åˆå§‹åŒ–
    # ======================================================
    final_model_path = os.path.join(MODELS_DIR, f"{FINAL_MODEL_NAME}.zip")

    if os.path.exists(final_model_path):
        print(f">>> â™»ï¸ å‘ç°ä¸Šæ¬¡ä¸­æ–­çš„è¿›åº¦ '{FINAL_MODEL_NAME}'ï¼Œæ­£åœ¨åŠ è½½ç»§ç»­è®­ç»ƒ...")
        # reset_num_timesteps=False è¡¨ç¤ºæ¥ä¸Šä¼ æ¬¡çš„æ—¶é—´æ­¥ç»§ç»­è®¡æ•°
        model = PPO.load(final_model_path, env=env)
    else:
        print(f">>> âœ¨ åˆ›å»ºå…¨æ–°çš„ PPO æ¨¡å‹...")
        model = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            tensorboard_log=LOG_DIR,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,  # ç†µç³»æ•°ï¼Œå¢åŠ æ¢ç´¢
            policy_kwargs=dict(
                net_arch=dict(pi=[128, 128], vf=[128, 128]),
                activation_fn=torch.nn.Tanh
            ),
        )

    # ======================================================
    # ğŸ“ å›è°ƒè®¾ç½® (è‡ªåŠ¨ä¿å­˜æœ€é«˜åˆ†)
    # ======================================================
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=MODELS_DIR,  # å‘ç°æ–°é«˜åˆ†æ—¶ï¼Œè‡ªåŠ¨å­˜åˆ°è¿™é‡Œ
        log_path=LOG_DIR,
        eval_freq=10000,  # æ¯ 1ä¸‡æ­¥ è¯„ä¼°ä¸€æ¬¡
        n_eval_episodes=5,  # æ¯æ¬¡è¯„ä¼°è·‘ 5 å±€å–å¹³å‡
        deterministic=True,  # è¯„ä¼°æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆä¸ä¹±è¯•ï¼‰
        render=False
    )

    # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šå‘Šè¯‰å›è°ƒå‡½æ•°ç›®å‰çš„æœ€é«˜çºªå½•æ˜¯å¤šå°‘
    # è¿™æ ·æ–°çš„ä¸€è½®è®­ç»ƒå¦‚æœåªæœ‰ 50 åˆ†ï¼Œå°±ä¸ä¼šè¦†ç›–æ‰ä¹‹å‰ 300 åˆ†çš„æ¨¡å‹
    eval_callback.best_mean_reward = historical_best_score

    # ======================================================
    # ğŸš€ å¼€å§‹å­¦ä¹ 
    # ======================================================
    try:
        model.learn(
            total_timesteps=TOTAL_TIMESTEPS,
            callback=eval_callback,
            progress_bar=True,
            reset_num_timesteps=False  # å¦‚æœæ˜¯ç»­è®­ï¼Œä¸é‡ç½®æ€»æ­¥æ•°
        )
        print(">>> âœ… è®­ç»ƒç›®æ ‡è¾¾æˆï¼")
    except KeyboardInterrupt:
        print("\n>>> ğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­è®­ç»ƒï¼æ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")

    # ======================================================
    # ğŸ’¾ ä¿å­˜â€œæœ€åä¸€æ¬¡â€çš„çŠ¶æ€ (æ— è®ºå¥½å)
    # ======================================================
    # è¿™ä¸ªæ–‡ä»¶ç”¨äºä¸‹æ¬¡ 'Resume' ç»§ç»­è®­ç»ƒ
    model.save(final_model_path)
    print(f">>> ğŸ’¾ è¿›åº¦å·²ä¿å­˜è‡³: {final_model_path}")
    print(f">>> ğŸŒŸ å†å²æœ€å¼ºæ¨¡å‹ (è¯·æµ‹è¯•è¿™ä¸ª): {best_model_path}")

    env.close()
    eval_env.close()


# ==========================================
# ğŸ® æµ‹è¯•/å±•ç¤ºå‡½æ•°
# ==========================================
def test():
    # æ°¸è¿œä¼˜å…ˆåŠ è½½ best_modelï¼Œå› ä¸ºé‚£æ‰æ˜¯æˆ‘ä»¬çš„å·…å³°
    load_path = os.path.join(MODELS_DIR, f"{BEST_MODEL_NAME}.zip")

    if not os.path.exists(load_path):
        print(f"âš ï¸ æ²¡æ‰¾åˆ°å·…å³°æ¨¡å‹ï¼Œå°è¯•åŠ è½½æœ€åä¸€æ¬¡çš„æ¨¡å‹: {FINAL_MODEL_NAME}.zip")
        load_path = os.path.join(MODELS_DIR, f"{FINAL_MODEL_NAME}.zip")

    if not os.path.exists(load_path):
        print("âŒ æ²¡æœ‰ä»»ä½•æ¨¡å‹æ–‡ä»¶ï¼è¯·å…ˆè¿è¡Œ train()ã€‚")
        return

    print(f"\n>>> ğŸ® æ­£åœ¨åŠ è½½æ¨¡å‹è¿›è¡Œæ¼”ç¤º: {load_path}")

    # å¼€å¯ render_mode='human' è®©ä½ çœ‹åˆ°ç”»é¢
    env = gym.make("FlappyBird-v0", render_mode="human", use_lidar=True)

    model = PPO.load(load_path)

    episodes = 5
    for ep in range(episodes):
        obs, _ = env.reset()
        done = False
        score = 0
        while not done:
            # å¿…é¡» deterministic=Trueï¼Œå¦åˆ™ AI ä¼šåœ¨è¿™ä¸ªé˜¶æ®µå°è¯•éšæœºåŠ¨ä½œå¯¼è‡´æ’æ­»
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            score = info.get('score', score)  # FlappyBird ç¯å¢ƒä¼šåœ¨ info é‡Œè¿”å› score

        print(f"ç¬¬ {ep + 1} å±€å¾—åˆ†: {score}")

    env.close()


if __name__ == "__main__":
    # ==========================================
    # ğŸ‘‡ æ§åˆ¶å¼€å…³ ğŸ‘‡
    # ==========================================

    # 1. æƒ³è¦è®­ç»ƒæ—¶ï¼Œè§£å¼€ä¸‹é¢è¿™è¡Œçš„æ³¨é‡Šï¼š
    train()

    # 2. æƒ³è¦çœ‹ AI ç©æ¸¸æˆæ—¶ï¼Œæ³¨é‡Šæ‰ä¸Šé¢çš„ train()ï¼Œè§£å¼€ä¸‹é¢è¿™è¡Œï¼š
    test()