import gymnasium as gym
import flappy_bird_gymnasium
import torch as th  # ğŸ”¥ ä¿®å¤ï¼šæ·»åŠ  torch å¯¼å…¥
from stable_baselines3 import DQN
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.results_plotter import load_results, ts2xy
import numpy as np
import os
import glob  # ğŸ”¥ ä¿®å¤ï¼šç”¨äºæŸ¥æ‰¾ monitor æ–‡ä»¶
import warnings
import matplotlib.pyplot as plt

# å¿½ç•¥ gymnasium çš„ç‰¹å®šè­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="gymnasium")

# ==========================================
# ğŸ¯ æ ¸å¿ƒé…ç½®
# ==========================================
MODELS_DIR = "models/flappy_bird_v2"
LOG_DIR = "logs/flappy_bird_v2"
MODEL_NAME = "flappy_bird_master"

# ğŸ”¥ å…³é”®1ï¼šä½¿ç”¨å‘é‡ç¯å¢ƒåŠ é€Ÿè®­ç»ƒ
N_ENVS = 4  # å¹¶è¡Œç¯å¢ƒæ•°é‡
# ğŸ”¥ å…³é”®2ï¼šå¥–åŠ±å¡‘é€ å¼€å…³
USE_SHAPED_REWARDS = True

os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


# ==========================================
# ğŸ å¥–åŠ±å¡‘é€ åŒ…è£…å™¨
# ==========================================
class FlappyBirdRewardShaper(gym.Wrapper):
    """ä¸ºFlappy Birdæ·»åŠ æ›´å¯†é›†çš„å¥–åŠ±ä¿¡å·"""

    def __init__(self, env):
        super().__init__(env)
        self.last_score = 0

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.last_score = 0
        return obs, info

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)

        if USE_SHAPED_REWARDS:
            # æå–å…³é”®çŠ¶æ€ä¿¡æ¯ (FlappyBird-v0 simple æ¨¡å¼)
            # obs[0]: bird_y
            # obs[1]: bird_vel
            # obs[2]: pipe_dist_x
            # obs[3]: pipe_top_y
            # obs[4]: pipe_bottom_y

            bird_y = obs[0]
            bird_vel = obs[1]
            pipe_dist_x = obs[2]
            pipe_top_y = obs[3]
            pipe_bottom_y = obs[4]
            gap_center = (pipe_top_y + pipe_bottom_y) / 2

            # 1. ç”Ÿå­˜å¥–åŠ±
            reward += 0.1

            # 2. é«˜åº¦ä¿æŒå¥–åŠ± (å½“é¸Ÿåœ¨ç®¡é“ä¹‹é—´æ—¶)
            # pipe_dist_x èŒƒå›´é€šå¸¸æ˜¯ [0, width]ï¼Œéœ€è¦ç¡®è®¤å…·ä½“æ•°å€¼èŒƒå›´ï¼Œè¿™é‡Œå‡è®¾æ ‡å‡†åŒ–å¤„ç†
            if pipe_dist_x > -0.5 and pipe_dist_x < 0.5:
                height_diff = abs(bird_y - gap_center)
                # è·ç¦»ä¸­å¿ƒè¶Šè¿‘ï¼Œå¥–åŠ±è¶Šé«˜ï¼Œæœ€å¤§ 0.5
                reward += max(0, 0.5 - height_diff)

            # 3. é€Ÿåº¦æƒ©ç½š (ç¨å¾®æŠ‘åˆ¶å‰§çƒˆæŠ–åŠ¨)
            reward -= abs(bird_vel) * 0.01

        return obs, reward, terminated, truncated, info


# ==========================================
# âš™ï¸ åŠ¨æ€å­¦ä¹ ç‡
# ==========================================
def linear_schedule(initial_value: float):
    """çº¿æ€§ä¸‹é™å­¦ä¹ ç‡"""

    def func(progress_remaining: float):
        return progress_remaining * initial_value

    return func


# ==========================================
# ğŸš€ è®­ç»ƒå‡½æ•°
# ==========================================
def train():
    print(f">>> [è®­ç»ƒ] åˆå§‹åŒ– {N_ENVS} ä¸ªå¹¶è¡Œç¯å¢ƒ...")

    # å®šä¹‰ç¯å¢ƒå·¥å‚å‡½æ•°
    def make_env():
        env = gym.make("FlappyBird-v0", render_mode=None, use_lidar=False)
        env = FlappyBirdRewardShaper(env)  # æ·»åŠ å¥–åŠ±å¡‘é€ 
        return env

    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ (Monitor ä¼šè‡ªåŠ¨æ·»åŠ åˆ°æ¯ä¸ªå­ç¯å¢ƒ)
    env = make_vec_env(make_env, n_envs=N_ENVS, monitor_dir=LOG_DIR)

    final_path = f"{MODELS_DIR}/{MODEL_NAME}.zip"
    ckpt_pattern = f"{MODELS_DIR}/ckpt_*_steps.zip"

    # --- æ¨¡å‹åŠ è½½é€»è¾‘ ---
    if os.path.exists(final_path):
        print(f">>> â™»ï¸ åŠ è½½æœ€ç»ˆæ¨¡å‹å¹¶ç»§ç»­è®­ç»ƒ...")
        model = DQN.load(final_path, env=env, tensorboard_log=LOG_DIR)
        start_steps = model.num_timesteps
        current_lr = 1e-5  # ç»§ç»­è®­ç»ƒä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡

    else:
        # æŸ¥æ‰¾æœ€è¿‘çš„ Checkpoint
        ckpts = glob.glob(ckpt_pattern)
        if ckpts:
            latest_ckpt = max(ckpts, key=os.path.getctime)
            print(f">>> â™»ï¸ åŠ è½½Checkpoint: {os.path.basename(latest_ckpt)}")
            model = DQN.load(latest_ckpt, env=env, tensorboard_log=LOG_DIR)
            start_steps = model.num_timesteps
        else:
            print(">>> ğŸ†• ä»é›¶å¼€å§‹è®­ç»ƒ...")
            start_steps = 0

            # ä¼˜åŒ–åçš„ç½‘ç»œç»“æ„
            policy_kwargs = dict(
                net_arch=[256, 256],
                activation_fn=th.nn.ReLU,  # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨ th.nn.ReLU
            )

            model = DQN(
                "MlpPolicy",
                env,
                verbose=0,
                tensorboard_log=LOG_DIR,
                learning_rate=linear_schedule(1e-4),
                buffer_size=500_000,
                learning_starts=10_000,
                batch_size=256,
                gamma=0.99,
                train_freq=4,
                gradient_steps=1,
                target_update_interval=1000,
                exploration_fraction=0.2,  # å‰20%æ—¶é—´æ¢ç´¢
                exploration_initial_eps=1.0,
                exploration_final_eps=0.05,
                policy_kwargs=policy_kwargs,
            )

    # --- å›è°ƒå‡½æ•° ---
    checkpoint_callback = CheckpointCallback(
        save_freq=100_000 // N_ENVS,
        save_path=MODELS_DIR,
        name_prefix="ckpt",
        save_replay_buffer=False,  # è®¾ä¸ºFalseä»¥èŠ‚çœç£ç›˜ç©ºé—´ï¼Œå¦‚æœæ˜¯Trueä¼šå¯¼è‡´ckptæ–‡ä»¶å·¨å¤§
        save_vecnormalize=True,
    )

    # è¯„ä¼°ç¯å¢ƒ (ä¸ºäº†å‡†ç¡®è¯„ä¼°ï¼Œå»ºè®®ä¸åŠ  RewardShaperï¼Œçœ‹çœŸå®åˆ†æ•°ï¼Œä½†ä¸ºäº†ä¿æŒè¾“å…¥ä¸€è‡´æ€§ï¼Œè¿™é‡Œä¿ç•™ç»“æ„)
    eval_env = make_vec_env(make_env, n_envs=1)

    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=f"{MODELS_DIR}/best_model",
        log_path=f"{LOG_DIR}/eval",
        eval_freq=50_000 // N_ENVS,
        n_eval_episodes=10,
        deterministic=True,
        render=False,
    )

    TOTAL_STEPS = 2_000_000  # è®¾ç½®ä¸º 2M æ­¥é€šå¸¸è¶³å¤Ÿå…¥é—¨
    steps_to_train = TOTAL_STEPS - start_steps

    if steps_to_train <= 0:
        print(">>> æ¨¡å‹å·²è¾¾åˆ°ç›®æ ‡æ­¥æ•°ï¼Œæ— éœ€è®­ç»ƒ")
        return

    print(f">>> [è®­ç»ƒ] ç›®æ ‡: {steps_to_train // 1000}k æ­¥ (æ€» {TOTAL_STEPS // 1000}k)...")

    try:
        model.learn(
            total_timesteps=steps_to_train,
            progress_bar=True,
            callback=[checkpoint_callback, eval_callback],
            tb_log_name="dqn_run",
            reset_num_timesteps=False,
        )
        model.save(f"{MODELS_DIR}/{MODEL_NAME}")
        print(f">>> [è®­ç»ƒ] å®Œæˆï¼å·²ä¿å­˜è‡³ {MODEL_NAME}")

    except KeyboardInterrupt:
        print("\n>>> [ä¸­æ–­] ä¿å­˜ç´§æ€¥å¤‡ä»½...")
        model.save(f"{MODELS_DIR}/interrupted_model")

    finally:
        env.close()
        eval_env.close()


# ==========================================
# ğŸ§ª æµ‹è¯•å‡½æ•°
# ==========================================
def test(episodes=5, deterministic=True):
    best_path = f"{MODELS_DIR}/best_model/best_model.zip"  # EvalCallback é€šå¸¸ä¿å­˜åœ¨å­æ–‡ä»¶å¤¹
    final_path = f"{MODELS_DIR}/{MODEL_NAME}.zip"

    load_path = None
    if os.path.exists(best_path):
        load_path = best_path
        print(f">>> [æµ‹è¯•] åŠ è½½æœ€ä½³æ¨¡å‹: {best_path}")
    elif os.path.exists(final_path):
        load_path = final_path
        print(f">>> [æµ‹è¯•] åŠ è½½æœ€ç»ˆæ¨¡å‹: {final_path}")
    else:
        # æŸ¥æ‰¾ ckpt
        ckpts = glob.glob(f"{MODELS_DIR}/ckpt_*.zip")
        if ckpts:
            load_path = max(ckpts, key=os.path.getctime)
            print(f">>> [æµ‹è¯•] åŠ è½½Checkpoint: {load_path}")

    if not load_path:
        print(">>> âŒ æœªæ‰¾åˆ°å¯åŠ è½½çš„æ¨¡å‹")
        return

    env = gym.make("FlappyBird-v0", render_mode="human", use_lidar=False)
    # æ³¨æ„ï¼šæµ‹è¯•æ—¶ä¸éœ€è¦ RewardShaperï¼Œæˆ‘ä»¬éœ€è¦çœ‹åŸå§‹åˆ†æ•°

    model = DQN.load(load_path)

    for ep in range(episodes):
        obs, _ = env.reset()
        done = False
        score = 0
        while not done:
            action, _ = model.predict(obs, deterministic=deterministic)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            # FlappyBird ç¯å¢ƒé€šå¸¸åœ¨ info ä¸­åŒ…å« 'score'
            score = info.get('score', score)

        print(f"Episode {ep + 1}: Score = {score}")

    env.close()


# ==========================================
# ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæ›²çº¿ (ä¿®å¤ç‰ˆ)
# ==========================================
def plot_results():
    print(">>> [ç»˜å›¾] æ­£åœ¨ç”Ÿæˆæ›²çº¿...")

    # ğŸ”¥ ä¿®å¤ï¼šmake_vec_env ä¼šç”Ÿæˆå¤šä¸ª monitor æ–‡ä»¶ (0.monitor.csv, 1.monitor.csv...)
    # è¿™é‡Œçš„é€»è¾‘æ˜¯è¯»å–æ‰€æœ‰æ–‡ä»¶å¹¶è®¡ç®—å¹³å‡å€¼ï¼Œæˆ–è€…åªè¯»å–ç¬¬ä¸€ä¸ª
    monitor_files = glob.glob(f"{LOG_DIR}/*.monitor.csv")

    if not monitor_files:
        print(">>> [è­¦å‘Š] æœªæ‰¾åˆ° Monitor CSV æ–‡ä»¶ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    try:
        # è¯»å–ç¬¬ä¸€ä¸ª monitor æ–‡ä»¶ (é€šå¸¸è¶³å¤Ÿä»£è¡¨è¶‹åŠ¿)
        # å¦‚æœéœ€è¦æ›´ç²¾ç¡®ï¼Œå¯ä»¥èšåˆæ‰€æœ‰ dataframe
        df = load_results(LOG_DIR)

        if len(df) < 2:
            print(">>> [è­¦å‘Š] æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•ç»˜å›¾ã€‚")
            return

        x, y = ts2xy(df, 'timesteps')

        # å¹³æ»‘å¤„ç†
        def moving_average(values, window):
            weights = np.repeat(1.0, window) / window
            return np.convolve(values, weights, 'valid')

        if len(y) > 100:
            y_smoothed = moving_average(y, window=50)
            x_smoothed = x[len(x) - len(y_smoothed):]
        else:
            y_smoothed, x_smoothed = y, x

        plt.figure(figsize=(10, 5))
        plt.plot(x_smoothed, y_smoothed, label="Smoothed Reward")
        plt.xlabel("Timesteps")
        plt.ylabel("Reward")
        plt.title("Flappy Bird Training Progress")
        plt.legend()
        plt.grid(True)

        save_path = f"{LOG_DIR}/training_curve.png"
        plt.savefig(save_path)
        print(f">>> [ç»˜å›¾] æ›²çº¿å·²ä¿å­˜è‡³: {save_path}")
        plt.close()

    except Exception as e:
        print(f">>> [é”™è¯¯] ç»˜å›¾å¤±è´¥: {e}")


if __name__ == "__main__":
    # 1. è®­ç»ƒ
    # train()

    # 2. æµ‹è¯•æ¨¡å‹ï¼ˆè®­ç»ƒå®Œæˆåï¼‰
    test(episodes=20, deterministic=True)

    # 3. ç»˜åˆ¶ç»“æœ
    plot_results()
