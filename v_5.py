import os
import time
import gymnasium as gym
import flappy_bird_gymnasium
import torch
import cv2
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor

# ==========================================
# âš™ï¸ å…¨å±€é…ç½®
# ==========================================
MODELS_DIR = "models/flappy_ppo_v3"
LOG_DIR = "logs/flappy_ppo_v3"
MODEL_NAME = "flappy_bird_ppo_best"
N_ENVS = 4

os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


def make_env():
    """åˆ›å»ºå•ä¸ª FlappyBird ç¯å¢ƒï¼ˆLIDAR æ¨¡å¼ï¼‰"""
    return gym.make("FlappyBird-v0", render_mode=None, use_lidar=True)


def train():
    print(f">>> [åˆå§‹åŒ–] å¯åŠ¨ {N_ENVS} ä¸ªå¹¶è¡Œç¯å¢ƒ (PPO)...")
    env = make_vec_env(make_env, n_envs=N_ENVS, monitor_dir=LOG_DIR)

    # ğŸ”§ æ¨èè¶…å‚æ•°ï¼ˆé’ˆå¯¹ Flappy Bird ç¨€ç–å¥–åŠ±ä¼˜åŒ–ï¼‰
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        tensorboard_log=LOG_DIR,

        # å­¦ä¹ ç‡ç¨ä½ï¼Œé¿å…åœ¨ç¨€ç–å¥–åŠ±ä¸‹éœ‡è¡
        learning_rate=2.5e-4,
        n_steps=2048,  # æ€» buffer = 2048 * 4 = 8192
        batch_size=128,  # å¢å¤§æ‰¹æ¬¡æå‡ç¨³å®šæ€§
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,  # ä¿æŒé€‚åº¦æ¢ç´¢

        policy_kwargs=dict(
            net_arch=dict(pi=[256, 256], vf=[256, 256]),  # ç¨å¤§ç½‘ç»œï¼Œåº”å¯¹184ç»´è¾“å…¥
            activation_fn=torch.nn.Tanh
        ),
        seed=42  # å¯å¤ç°æ€§
    )

    # ğŸ”„ æ–­ç‚¹ç»­è®­
    final_path = os.path.join(MODELS_DIR, f"{MODEL_NAME}.zip")
    if os.path.exists(final_path):
        print(">>> â™»ï¸ åŠ è½½å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ...")
        model = PPO.load(final_path, env=env, tensorboard_log=LOG_DIR)

    # ğŸ“Š è¯„ä¼°å›è°ƒï¼ˆæ¯ 5k æ­¥è¯„ä¼°ä¸€æ¬¡ï¼Œæ›´å¿«åé¦ˆï¼‰
    eval_env = make_vec_env(make_env, n_envs=1)
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=MODELS_DIR,
        log_path=LOG_DIR,
        eval_freq=5000,  # æ›´é¢‘ç¹è¯„ä¼°
        n_eval_episodes=5,
        deterministic=True,
        render=False
    )

    TOTAL_STEPS = 1_000_000
    print(f">>> [å¼€å§‹è®­ç»ƒ] ç›®æ ‡æ­¥æ•°: {TOTAL_STEPS}")

    try:
        model.learn(
            total_timesteps=TOTAL_STEPS,
            callback=eval_callback,
            progress_bar=True
        )
        model.save(final_path)
        print(">>> âœ… è®­ç»ƒå®Œæˆï¼")

    except KeyboardInterrupt:
        print(">>> ğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰æ¨¡å‹...")
        model.save(os.path.join(MODELS_DIR, "interrupted_ppo"))
    finally:
        env.close()


def test(render=True, episodes=10):
    """æµ‹è¯•æ¨¡å‹ï¼Œæ”¯æŒæ¸²æŸ“å’Œæ­»äº¡æˆªå›¾"""
    best_path = os.path.join(MODELS_DIR, "best_model.zip")
    final_path = os.path.join(MODELS_DIR, f"{MODEL_NAME}.zip")

    model_path = best_path if os.path.exists(best_path) else final_path
    if not os.path.exists(model_path):
        raise FileNotFoundError("âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼è¯·å…ˆè®­ç»ƒã€‚")

    print(f">>> ğŸ® åŠ è½½æ¨¡å‹: {model_path}")

    # æ ¹æ®æ˜¯å¦æ¸²æŸ“é€‰æ‹© render_mode
    render_mode = "rgb_array" if render else None
    env = gym.make("FlappyBird-v0", render_mode=render_mode, use_lidar=True)
    model = PPO.load(model_path)

    for ep in range(episodes):
        obs, _ = env.reset()
        done = False
        score = 0
        last_frame = None

        while not done:
            if render:
                frame = env.render()
                last_frame = frame
                bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                cv2.imshow("Flappy Bird Replay", bgr)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            score = info.get('score', score)

        if render and last_frame is not None:
            timestamp = int(time.time())
            filename = os.path.join(LOG_DIR, f"death_ep{ep + 1}_score{score}_{timestamp}.png")
            cv2.imwrite(filename, cv2.cvtColor(last_frame, cv2.COLOR_RGB2BGR))
            print(f"ğŸ’€ ç¬¬ {ep + 1} å±€ç»“æŸ (åˆ†: {score}) â†’ æˆªå›¾: {filename}")

    env.close()
    if render:
        cv2.destroyAllWindows()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--train", action="store_true", help="å¯åŠ¨è®­ç»ƒ")
    parser.add_argument("--test", action="store_true", help="å¯åŠ¨æµ‹è¯•")
    parser.add_argument("--no-render", action="store_true", help="æµ‹è¯•æ—¶ä¸æ¸²æŸ“ç”»é¢")
    args = parser.parse_args()

    if args.train:
        train()
    elif args.test:
        test(render=not args.no_render)
    else:
        print("ç”¨æ³•: python flappy_ppo.py --train   # å¼€å§‹è®­ç»ƒ")
        print("      python flappy_ppo.py --test    # è§‚çœ‹ AI ç©æ¸¸æˆ")
        print("      python flappy_ppo.py --test --no-render  # æ— æ¸²æŸ“æµ‹è¯•ï¼ˆä»…æ‰“å°åˆ†æ•°ï¼‰")