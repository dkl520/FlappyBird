import os
import gymnasium as gym
import flappy_bird_gymnasium
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from sb3_contrib import MaskablePPO  # å¦‚æœæ²¡æœ‰å®‰è£… sb3_contribï¼Œå¯ä»¥ç”¨æ™®é€šçš„ PPO
import torch

# ================= ğŸš€ é…ç½®åŒºåŸŸ =================
# å…³é”®ç‚¹ï¼šå¼€å¯ use_lidar=Trueï¼Œè¿™æ˜¯æ‹¿é«˜åˆ†çš„æ ¸å¿ƒ
ENV_ID = "FlappyBird-v0"
MODEL_DIR = "trained_models"
LOG_DIR = "logs"
MODEL_NAME = "FlappyBird_Master"
TOTAL_TIMESTEPS = 1_000_000  # å»ºè®®è®­ç»ƒ 50ä¸‡-100ä¸‡æ­¥

# åˆ›å»ºç›®å½•
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


def make_env(render_mode=None):
    """
    åˆ›å»ºç¯å¢ƒçš„è¾…åŠ©å‡½æ•°
    use_lidar=True: å¼€å¯æ¿€å…‰é›·è¾¾ï¼Œå¤§å¤§é™ä½è®­ç»ƒéš¾åº¦
    """

    def _init():
        env = gym.make(
            ENV_ID,
            render_mode=render_mode,
            use_lidar=True,  # <--- æ ¸å¿ƒï¼šå¼€å¯é›·è¾¾
            background=None  # å…³é—­èƒŒæ™¯ä»¥åŠ é€Ÿè®­ç»ƒ(å¯é€‰)
        )
        return env

    return _init


def train():
    print("ğŸš€ å¼€å§‹è®­ç»ƒå¤§å¸ˆçº§æ¨¡å‹...")
    print(f"ğŸ“Œ è®¾å¤‡: {'GPU (cuda)' if torch.cuda.is_available() else 'CPU'}")

    # ä½¿ç”¨å¤šè¿›ç¨‹ç¯å¢ƒåŠ é€Ÿè®­ç»ƒ (4ä¸ªè¿›ç¨‹)
    # å¦‚æœæŠ¥é”™ï¼Œå¯ä»¥æ”¹å› DummyVecEnv([make_env()])
    num_cpu = 4
    env = SubprocVecEnv([make_env() for _ in range(num_cpu)])

    # å®šä¹‰ PPO æ¨¡å‹å‚æ•° (ç»è¿‡ä¼˜åŒ–çš„å‚æ•°)
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        learning_rate=2.5e-4,  # ç¨å¾®é™ä½å­¦ä¹ ç‡
        n_steps=2048,
        batch_size=64,
        n_epochs=10,  # æ¯æ¬¡æ›´æ–°å¤šå­¦å‡ é
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,  # ç†µç³»æ•°ï¼Œé¼“åŠ±æ¢ç´¢
        tensorboard_log=LOG_DIR,  # å¯è§†åŒ–æ—¥å¿—
        device="auto"
    )

    # è‡ªåŠ¨ä¿å­˜å›è°ƒï¼šæ¯ 10ä¸‡æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    checkpoint_callback = CheckpointCallback(
        save_freq=100_000 // num_cpu,
        save_path=MODEL_DIR,
        name_prefix="ppo_flappy"
    )

    try:
        model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=checkpoint_callback, progress_bar=True)
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = os.path.join(MODEL_DIR, MODEL_NAME)
        model.save(final_path)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {final_path}")
    except KeyboardInterrupt:
        print("âš ï¸ è®­ç»ƒæ‰‹åŠ¨åœæ­¢ï¼Œæ­£åœ¨ä¿å­˜å½“å‰æ¨¡å‹...")
        model.save(os.path.join(MODEL_DIR, "interrupted_model"))
    finally:
        env.close()


def test():
    print("ğŸ‘€ æ­£åœ¨åŠ è½½å¤§å¸ˆçº§æ¨¡å‹è¿›è¡Œæ¼”ç¤º...")

    model_path = os.path.join(MODEL_DIR, MODEL_NAME + ".zip")
    if not os.path.exists(model_path):
        # å°è¯•å¯»æ‰¾ä¸­é—´ä¿å­˜çš„æ¨¡å‹
        print(f"âŒ æ‰¾ä¸åˆ° {model_path}ï¼Œè¯·æ£€æŸ¥æ˜¯å¦è®­ç»ƒå®Œæˆã€‚")
        return

    # æµ‹è¯•æ—¶å¼€å¯æ¸²æŸ“ï¼Œå¹¶ä¸”å¿…é¡»ä¿æŒ use_lidar=True
    env = gym.make(ENV_ID, render_mode="human", use_lidar=True)

    model = PPO.load(model_path, env=env)

    episodes = 5
    for ep in range(episodes):
        obs, _ = env.reset()
        done = False
        total_reward = 0

        print(f"ğŸ¬ Episode {ep + 1} å¼€å§‹...")
        while not done:
            # deterministic=True è®©åŠ¨ä½œæ›´ç¨³å®š
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated
            if truncated:
                pass
            total_reward += reward

            # å¦‚æœåˆ†æ•°å¤ªé«˜ï¼Œä¸æƒ³çœ‹äº†å¯ä»¥æŒ‰ Ctrl+C
            if info['score'] > 1000:
                print("âœ¨ åˆ†æ•°è¶…è¿‡1000ï¼Œå¤ªå¼ºäº†ï¼Œè‡ªåŠ¨è·³è¿‡...")
                # break

        print(f"ğŸ Episode {ep + 1} å¾—åˆ†: {info['score']}")

    env.close()


if __name__ == "__main__":
    # 1. å…ˆè¿è¡Œ train()
    # 2. è®­ç»ƒå®Œåæ³¨é‡Šæ‰ train()ï¼Œè¿è¡Œ test()

    # train()
    test()
